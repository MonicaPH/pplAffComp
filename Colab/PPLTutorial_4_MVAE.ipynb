{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPLTutorial_4_MVAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desmond-ong/pplAffComp/blob/master/Colab/PPLTutorial_4_MVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwIJxjvwS07H",
        "colab_type": "text"
      },
      "source": [
        "# Extensions to a Multimodal VAE\n",
        "\n",
        "Another way that the VAE model can be extended is to fully let the latent $z$ \"cause\" both the emotion ratings and the facial expression. This is an example of a Multimodal VAE (Wu & Goodman, 2018).\n",
        "\n",
        "There is a nice theoretical motivation for this model too. Throughout the past few examples, we've assumed that the space of emotions is exactly what we measured (e.g., some value of happiness, some value of sadness), but maybe the latent space is structured not along these discrete emotion categories, but perhaps along dimensions like \"good\" vs \"bad\", or some other semantic property. In emotion theory, this undifferentiated space is called affect, and often, this is low-dimensional (2 to 3 dimensions capture most of the variance in empirical data -- although in practice we often set our latent dimension parameter to be higher in these latent variable models).\n",
        "\n",
        "We could thus posit a latent *affect* that generates the emotion ratings. That is, the emotion ratings is some projection of the latent affect space onto these emotion concepts that are given meaning by the language and culture that one resides in.\n",
        "\n",
        "But in fact, we would still want a latent $z$ that captures non-emotional aspects of the face. For simplicity, for this example, we assume that this latent $z$ will capture some aspects of affect as well as the face. Learning to disentangle latent variables (e.g. the latent variables that are important for emotions and the latent variables that are not) is also an active area of research (Narayanaswamy et al, 2017).\n",
        "\n",
        "And finally, we can add the \"outcome to appraisal to affect\" part back into this multimodal model.\n",
        "\n",
        "<div style=\"width: 300px; margin: auto;\"> <center>\n",
        "<img src=\"https://desmond-ong.github.io/pplAffComp/code/images/graphicalModel_MVAE.png\" width=300></img> </center>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNALUpARUUZT",
        "colab_type": "text"
      },
      "source": [
        "## Preamble\n",
        "\n",
        "These first chunks of code clones the GitHub repo, installs Pyro and other requirements on the Colab server (if not already installed), and imports the necessary python packages and functions that we will use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6xfoTzIUTtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/desmond-ong/pplAffComp.git\n",
        "%cd pplAffComp/code\n",
        "!pip install torch torchvision pyro-ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znO3mZ_-SSwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib # for urllib.urlretrieve()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchvision.transforms import ToPILImage\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from pyro.contrib.examples.util import print_and_log\n",
        "import pyro.poutine as poutine\n",
        "# custom helperCode for this tutorial, in helperCode.py\n",
        "import helperCode\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "print(\"Currently using Pyro version: \" + pyro.__version__)\n",
        "\n",
        "#IMG_WIDTH = 100\n",
        "# Note that we downsample to 64 x 64 here, because we wanted a nice power of 2 \n",
        "#(and DCGAN architecture assumes input image of 64x64) \n",
        "\n",
        "IMG_WIDTH = 64\n",
        "IMG_SIZE = IMG_WIDTH*IMG_WIDTH*3\n",
        "BATCH_SIZE = 32\n",
        "DEFAULT_Z_DIM = 50\n",
        "\n",
        "OUTCOME_VAR_NAMES = ['payoff1', 'payoff2', 'payoff3', \n",
        "                     'prob1', 'prob2', 'prob3', \n",
        "                     'win', 'winProb', 'angleProp']\n",
        "EMOTION_VAR_NAMES = ['happy', 'sad', 'anger', 'surprise', \n",
        "                     'disgust', 'fear', 'content', 'disapp']\n",
        "\n",
        "OUTCOME_VAR_DIM = len(OUTCOME_VAR_NAMES)\n",
        "EMOTION_VAR_DIM = len(EMOTION_VAR_NAMES)\n",
        "\n",
        "# helper functions\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6zP2u3QS8WT",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset\n",
        "\n",
        "(This part is the same as the SSVAE Tutorial)\n",
        "\n",
        "We will be using the same dataset as the previous examples. We will consider the trials in which participants only saw a facial expression, and rated how they thought the character feels, or what we call the \"facial expression only\" trials.\n",
        "\n",
        "Here is a preview of the 18 faces (which are in ../CognitionData/faces/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77np7ddnSyrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faces_path = os.path.join(os.path.abspath('..'), \"CognitionData\", \"faces\")\n",
        "\n",
        "# initializing two temp arrays\n",
        "faceArray1 = np.zeros(shape=(100,1,3), dtype='uint8')\n",
        "faceArray2 = np.zeros(shape=(100,1,3), dtype='uint8')\n",
        "\n",
        "count = 0\n",
        "for thisFace in helperCode.FACE_FILENAMES:\n",
        "    newFaceArray = np.array(Image.open(os.path.join(faces_path, thisFace + \".png\")))\n",
        "    if count < 6 or count > 14:\n",
        "        faceArray1 = np.concatenate((faceArray1, newFaceArray), axis=1)\n",
        "    else:\n",
        "        faceArray2 = np.concatenate((faceArray2, newFaceArray), axis=1)\n",
        "    count += 1\n",
        "\n",
        "# concatenating the arrays and removing the first temp column\n",
        "faceArray = np.concatenate((faceArray1, faceArray2), axis=0)\n",
        "faceArray = faceArray[:,1:,:]\n",
        "Image.fromarray(faceArray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrZNDrVITBj_",
        "colab_type": "text"
      },
      "source": [
        "This next chunk defines a function to read in the data, and stores the data in `face_outcome_emotion_dataset`. There are N=1,587 observations, and each observation consists of:\n",
        "\n",
        "- an accompanying face image\n",
        "- a 9-dimension outcome vector that parameterizes the gamble that agents played, and\n",
        "- an 8-dimensional emotion rating vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4pFzIY-SzG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data location\n",
        "dataset_path = os.path.join(os.path.abspath('..'), \"CognitionData\", \"data_faceWheel.csv\")\n",
        "\n",
        "class FaceOutcomeEmotionDataset(Dataset):\n",
        "    \"\"\"Face Outcome Emotion dataset.\"\"\"\n",
        "    \n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the experiment csv file \n",
        "            img_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.expdata = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        ## Normalizing the data:\n",
        "        ####\n",
        "        ## payoff1, payoff2, payoff3 and win are between 0 and 100\n",
        "        ## need to normalize to [0,1] to match the rest of the variables,\n",
        "        ## by dividing payoff1, payoff2, payoff3 and win by 100\n",
        "        ####\n",
        "        self.expdata.loc[:,\"payoff1\"] = self.expdata.loc[:,\"payoff1\"]/100\n",
        "        self.expdata.loc[:,\"payoff2\"] = self.expdata.loc[:,\"payoff2\"]/100\n",
        "        self.expdata.loc[:,\"payoff3\"] = self.expdata.loc[:,\"payoff3\"]/100\n",
        "        self.expdata.loc[:,\"win\"]     = self.expdata.loc[:,\"win\"]/100\n",
        "        # Emotions were rated on a 1-9 Likert scale.\n",
        "        # use emo <- (emo-1)/8 to transform to within [0,1]\n",
        "        self.expdata.loc[:,\"happy\":\"disapp\"] = (self.expdata.loc[:,\"happy\":\"disapp\"]-1)/8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.expdata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ratings = np.array(self.expdata.iloc[idx][\"happy\":\"disapp\"], np.float32)\n",
        "        \n",
        "        outcomes = np.array(self.expdata.iloc[idx][\"payoff1\":\"angleProp\"], np.float32)\n",
        "        \n",
        "        img_name = os.path.join(self.img_dir, self.expdata.iloc[idx][\"facePath\"] + \".png\")\n",
        "        try:\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "            \n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        except:\n",
        "            print(img_name)\n",
        "            raise\n",
        "\n",
        "        return image, ratings, outcomes\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    # Note that we downsample to 64 x 64 here, because we wanted a nice power of 2 \n",
        "    #(and DCGAN architecture assumes input image of 64x64) \n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "# reads in datafile.\n",
        "print(\"Reading in dataset...\")\n",
        "\n",
        "face_outcome_emotion_dataset = FaceOutcomeEmotionDataset(csv_file=dataset_path, \n",
        "                                                         img_dir=faces_path, \n",
        "                                                         transform=data_transform)\n",
        "face_outcome_emotion_loader = torch.utils.data.DataLoader(face_outcome_emotion_dataset,\n",
        "                                                          batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                                          num_workers=4)\n",
        "\n",
        "N_samples = len(face_outcome_emotion_dataset)\n",
        "print(\"Number of observations:\", N_samples)\n",
        "\n",
        "# taking a sample observation\n",
        "img1, emo1, out1 = face_outcome_emotion_dataset[5]\n",
        "print(\"Sample Observation: \")\n",
        "print(helperCode.EMOTION_VAR_NAMES)\n",
        "print(emo1)\n",
        "print(helperCode.OUTCOME_VAR_NAMES)\n",
        "print(out1)\n",
        "Image.fromarray(helperCode.TensorToPILImage(img1*255.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ry4O5HTGbG",
        "colab_type": "text"
      },
      "source": [
        "### Defining the encoder and decoder networks:\n",
        "\n",
        "Here, we define the `Encoder` and `Decoder` modules, basically the two neural networks that go from some modality to $z$, and $z$ to image, respectively.\n",
        "\n",
        "This is the same as the VAE and SSVAE, just that here, in the *Multimodal* VAE, we have several of these networks. For example, an ImageEncoder/Decoder that we borrow the DCGAN architecture, a Rating Encoder/Decoder and an Outcome Encoder/Decoder. (It's not completely optimized or parameterized, there are a lot of hard-coded decisions here)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9270fNXJSzPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ProductOfExperts(nn.Module):\n",
        "    \"\"\"\n",
        "    Return parameters for product of independent experts.\n",
        "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
        "\n",
        "    @param loc: M x D for M experts\n",
        "    @param scale: M x D for M experts\n",
        "    \"\"\"\n",
        "    def forward(self, loc, scale, eps=1e-8):\n",
        "        scale = scale + eps # numerical constant for stability\n",
        "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
        "        T = 1. / scale\n",
        "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
        "        product_scale = 1. / torch.sum(T, dim=0)\n",
        "        return product_loc, product_scale\n",
        "      \n",
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes q(z|image).\n",
        "    This goes from images to the latent z\n",
        "    \n",
        "    This is the standard DCGAN architecture.\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
        "        #                padding=0, dilation=1, groups=1, bias=True)\n",
        "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
        "            Swish(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            Swish(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            Swish(),\n",
        "            nn.Conv2d(128, 256, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            Swish())\n",
        "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
        "        self.z_loc_layer = nn.Sequential(\n",
        "            nn.Linear(256 * 5 * 5, 512), # it's 256 * 5 * 5 if input is 64x64.\n",
        "            Swish(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_scale_layer = nn.Sequential(\n",
        "            nn.Linear(256 * 5 * 5, 512), # it's 256 * 5 * 5 if input is 64x64.\n",
        "            Swish(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    def forward(self, image):\n",
        "        hidden = self.features(image)\n",
        "        hidden = hidden.view(-1, 256 * 5 * 5) # it's 256 * 5 * 5 if input is 64x64.\n",
        "        z_loc = self.z_loc_layer(hidden)\n",
        "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
        "        return z_loc, z_scale\n",
        "    \n",
        "class ImageDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes p(image|z).\n",
        "    This goes from the latent z to the images\n",
        "    \n",
        "    This is the standard DCGAN architecture.\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(ImageDecoder, self).__init__()\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256 * 5 * 5),  # it's 256 * 5 * 5 if input is 64x64.\n",
        "            Swish())\n",
        "        self.hallucinate = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            Swish(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            Swish(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            Swish(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False))\n",
        "\n",
        "    def forward(self, z):\n",
        "        # the input will be a vector of size |z_dim|\n",
        "        z = self.upsample(z)\n",
        "        z = z.view(-1, 256, 5, 5) # it's 256 * 5 * 5 if input is 64x64.\n",
        "        image = self.hallucinate(z) # this is the image\n",
        "        return image  # NOTE: no sigmoid here. See train.py\n",
        "\n",
        "      \n",
        "class RatingEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes q(z|rating).\n",
        "    This goes from ratings to the latent z\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(RatingEncoder, self).__init__()\n",
        "        self.net = nn.Linear(helperCode.EMOTION_VAR_DIM, 512)\n",
        "        self.z_loc_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_scale_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    def forward(self, rating):\n",
        "        hidden = self.net(rating)\n",
        "        z_loc = self.z_loc_layer(hidden)\n",
        "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
        "        return z_loc, z_scale\n",
        "\n",
        "\n",
        "class RatingDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes p(rating|z).\n",
        "    This goes from the latent z to the ratings\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(RatingDecoder, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 512),\n",
        "            Swish())\n",
        "        self.rating_loc_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, helperCode.EMOTION_VAR_DIM))\n",
        "        self.rating_scale_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, helperCode.EMOTION_VAR_DIM))\n",
        "\n",
        "    def forward(self, z):\n",
        "        #batch_size = z.size(0)\n",
        "        hidden = self.net(z)\n",
        "        rating_loc = self.rating_loc_layer(hidden)\n",
        "        rating_scale = torch.exp(self.rating_scale_layer(hidden))\n",
        "        return rating_loc, rating_scale  # NOTE: no softmax here. See train.py\n",
        "\n",
        "\n",
        "class OutcomeEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes q(z|outcome).\n",
        "    This goes from outcomes to the latent z\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(OutcomeEncoder, self).__init__()\n",
        "        self.net = nn.Linear(helperCode.OUTCOME_VAR_DIM, 512)\n",
        "        self.z_loc_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_scale_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, z_dim))\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    def forward(self, outcomes):\n",
        "        hidden = self.net(outcomes)\n",
        "        z_loc = self.z_loc_layer(hidden)\n",
        "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
        "        return z_loc, z_scale\n",
        "\n",
        "\n",
        "class OutcomeDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    define the PyTorch module that parametrizes p(outcomes|z).\n",
        "    This goes from the latent z to the outcomes\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        super(OutcomeDecoder, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 512),\n",
        "            Swish())\n",
        "        self.outcome_loc_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, helperCode.OUTCOME_VAR_DIM))\n",
        "        self.outcome_scale_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            Swish(),\n",
        "            nn.Linear(512, helperCode.OUTCOME_VAR_DIM))\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        hidden = self.net(z)\n",
        "        outcome_loc = self.outcome_loc_layer(hidden)\n",
        "        outcome_scale = torch.exp(self.outcome_scale_layer(hidden))\n",
        "        return outcome_loc, outcome_scale  # no nonlinearity here | will be added later\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVTalylSVS_G",
        "colab_type": "text"
      },
      "source": [
        "### MVAE Module\n",
        "Here we define the MVAE Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Gg9H8XSzRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
        "    multimodal variational auto-encoder.\n",
        "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
        "    Multimodal Variational Autoencoder.\n",
        "\n",
        "    @param z_dim: integer\n",
        "                  size of the tensor representing the latent random variable z\n",
        "                  \n",
        "    Currently all the neural network dimensions are hard-coded; \n",
        "    in a future version will make them be inputs into the constructor\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, use_cuda=False):\n",
        "        super(MVAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.image_encoder = ImageEncoder(z_dim)\n",
        "        self.image_decoder = ImageDecoder(z_dim)\n",
        "        self.rating_encoder = RatingEncoder(z_dim)\n",
        "        self.rating_decoder = RatingDecoder(z_dim)\n",
        "        self.outcome_encoder = OutcomeEncoder(z_dim)\n",
        "        self.outcome_decoder = OutcomeDecoder(z_dim)\n",
        "        self.experts = ProductOfExperts()\n",
        "        self.use_cuda = use_cuda\n",
        "        # relative weights of losses in the different modalities\n",
        "        self.LAMBDA_IMAGES = 1.0\n",
        "        self.LAMBDA_RATINGS = 50.0\n",
        "        self.LAMBDA_OUTCOMES = 50.0\n",
        "        \n",
        "        # using GPUs for faster training of the networks\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "    \n",
        "    def model(self, images=None, ratings=None, outcomes=None, annealing_beta=1.0):\n",
        "        # register this pytorch module and all of its sub-modules with pyro\n",
        "        pyro.module(\"mvae\", self)\n",
        "        \n",
        "        batch_size = 0\n",
        "        if images is not None:\n",
        "            batch_size = images.size(0)\n",
        "        elif ratings is not None:\n",
        "            batch_size = ratings.size(0)\n",
        "        elif outcomes is not None:\n",
        "            batch_size = outcomes.size(0)\n",
        "        \n",
        "        with pyro.plate(\"data\"):                        \n",
        "            # sample from outcome prior N(0.5,0.1), compute p(z|outcome)\n",
        "            outcome_prior_loc = torch.zeros(torch.Size((batch_size, helperCode.OUTCOME_VAR_DIM))) + 0.5\n",
        "            outcome_prior_scale = torch.ones(torch.Size((batch_size, helperCode.OUTCOME_VAR_DIM))) * 0.1\n",
        "            if outcomes is not None:\n",
        "                # if outcome is provided as an observed input, score against it\n",
        "                with poutine.scale(scale=self.LAMBDA_OUTCOMES):\n",
        "                    pyro.sample(\"obs_outcome\", dist.Normal(outcome_prior_loc, outcome_prior_scale), obs=outcomes)\n",
        "            else:\n",
        "                # else if outcome is not provided, just sample from priors\n",
        "                with poutine.scale(scale=self.LAMBDA_OUTCOMES):\n",
        "                    outcomes = pyro.sample(\"obs_outcome\", dist.Normal(outcome_prior_loc, outcome_prior_scale))\n",
        "            \n",
        "            z_loc, z_scale = self.outcome_encoder.forward(outcomes)\n",
        "            \n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            with poutine.scale(scale=annealing_beta):\n",
        "                z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale))\n",
        "\n",
        "            # decode the latent code z\n",
        "            img_loc = self.image_decoder.forward(z)\n",
        "            # score against actual images\n",
        "            if images is not None:\n",
        "                with poutine.scale(scale=self.LAMBDA_IMAGES):\n",
        "                    pyro.sample(\"obs_img\", dist.Bernoulli(img_loc), obs=images)\n",
        "            \n",
        "            rating_loc, rating_scale = self.rating_decoder.forward(z)\n",
        "            if ratings is not None:\n",
        "                with poutine.scale(scale=self.LAMBDA_RATINGS):\n",
        "                    pyro.sample(\"obs_rating\", dist.Normal(rating_loc, rating_scale), obs=ratings)\n",
        "\n",
        "            # return the loc so we can visualize it later\n",
        "            return img_loc, rating_loc\n",
        "    \n",
        "    def guide(self, images=None, ratings=None, outcomes=None, annealing_beta=1.0):\n",
        "        # register this pytorch module and all of its sub-modules with pyro\n",
        "        pyro.module(\"mvae\", self)\n",
        "        \n",
        "        batch_size = 0\n",
        "        if images is not None:\n",
        "            batch_size = images.size(0)\n",
        "        elif ratings is not None:\n",
        "            batch_size = ratings.size(0)\n",
        "        elif outcomes is not None:\n",
        "            batch_size = outcomes.size(0)\n",
        "            \n",
        "        with pyro.plate(\"data\"):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "                        \n",
        "            # initialize the prior expert.\n",
        "            # we initalize an additional dimension, along which we concatenate all the \n",
        "            #   different experts.\n",
        "            # self.experts() then combines the information from these different modalities\n",
        "            #   by multiplying the gaussians together\n",
        "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
        "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
        "            if self.use_cuda:\n",
        "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
        "            \n",
        "            if outcomes is not None:\n",
        "                outcome_z_loc, outcome_z_scale = self.outcome_encoder.forward(outcomes)\n",
        "                z_loc = torch.cat((z_loc, outcome_z_loc.unsqueeze(0)), dim=0)\n",
        "                z_scale = torch.cat((z_scale, outcome_z_scale.unsqueeze(0)), dim=0)\n",
        "                \n",
        "            if images is not None:\n",
        "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
        "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
        "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
        "            \n",
        "            if ratings is not None:\n",
        "                rating_z_loc, rating_z_scale = self.rating_encoder.forward(ratings)\n",
        "                z_loc = torch.cat((z_loc, rating_z_loc.unsqueeze(0)), dim=0)\n",
        "                z_scale = torch.cat((z_scale, rating_z_scale.unsqueeze(0)), dim=0)\n",
        "            \n",
        "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
        "            # sample the latent z\n",
        "            with poutine.scale(scale=annealing_beta):\n",
        "                pyro.sample(\"latent\", dist.Normal(z_loc, z_scale))\n",
        "    \n",
        "    def forward(self, image=None, rating=None, outcome=None):\n",
        "        z_loc, z_scale  = self.infer(image, rating, outcome)\n",
        "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
        "        # reconstruct inputs based on that gaussian\n",
        "        image_recon = self.image_decoder(z)\n",
        "        rating_recon = self.rating_decoder(z)\n",
        "        outcome_recon = self.outcome_decoder(z)\n",
        "        return image_recon, rating_recon, outcome_recon, z_loc, z_scale\n",
        "\n",
        "    def infer(self, images=None, ratings=None, outcomes=None):\n",
        "        batch_size = 0\n",
        "        if images is not None:\n",
        "            batch_size = images.size(0)\n",
        "        elif ratings is not None:\n",
        "            batch_size = ratings.size(0)\n",
        "        elif outcomes is not None:\n",
        "            batch_size = outcomes.size(0)\n",
        "            \n",
        "        # initialize the prior expert\n",
        "        # we initalize an additional dimension, along which we concatenate all the \n",
        "        #   different experts.\n",
        "        # self.experts() then combines the information from these different modalities\n",
        "        #   by multiplying the gaussians together\n",
        "        z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim))) + 0.5\n",
        "        z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim))) * 0.1\n",
        "        if self.use_cuda:\n",
        "            z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
        "\n",
        "        if outcomes is not None:\n",
        "            outcome_z_loc, outcome_z_scale = self.outcome_encoder.forward(outcomes)\n",
        "            z_loc = torch.cat((z_loc, outcome_z_loc.unsqueeze(0)), dim=0)\n",
        "            z_scale = torch.cat((z_scale, outcome_z_scale.unsqueeze(0)), dim=0)\n",
        "\n",
        "        if images is not None:\n",
        "            image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
        "            z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
        "            z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
        "\n",
        "        if ratings is not None:\n",
        "            rating_z_loc, rating_z_scale = self.rating_encoder.forward(ratings)\n",
        "            z_loc = torch.cat((z_loc, rating_z_loc.unsqueeze(0)), dim=0)\n",
        "            z_scale = torch.cat((z_scale, rating_z_scale.unsqueeze(0)), dim=0)\n",
        "\n",
        "        z_loc, z_scale = self.experts(z_loc, z_scale)\n",
        "        return z_loc, z_scale\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, images):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.image_encoder(images)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        img_loc = self.image_decoder.forward(z)\n",
        "        return img_loc\n",
        "\n",
        "    # define a helper function for reconstructing images without sampling\n",
        "    def reconstruct_img_nosample(self, images):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.image_encoder(images)\n",
        "        ## sample in latent space\n",
        "        #z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        img_loc = self.image_decoder.forward(z_loc)\n",
        "        return img_loc\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSIKLP7cTX7O",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "Next we define the parameters of our training session, and set up the model and inference algorithm.\n",
        "\n",
        "Since the training takes a while, `num_epochs` below is set to 2 just to demonstrate training. Ideally you'll want to train for much longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tck7DuKWE-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyro.clear_param_store()\n",
        "\n",
        "class Args:\n",
        "    learning_rate = 5e-6\n",
        "    num_epochs = 2 #500\n",
        "    z_dim = DEFAULT_Z_DIM\n",
        "    seed = 30\n",
        "    cuda = False\n",
        "    \n",
        "args = Args()\n",
        "\n",
        "# setup the VAE\n",
        "mvae = MVAE(z_dim=args.z_dim, use_cuda=args.cuda)\n",
        "\n",
        "# setup the optimizer\n",
        "adam_args = {\"lr\": args.learning_rate}\n",
        "optimizer = Adam(adam_args)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBE_5lFWQfu",
        "colab_type": "text"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "This next chunk of code runs the training over `num_epochs` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaZxLVOSSzUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_elbo = []\n",
        "trainingTimes = [time.time()]\n",
        "# training loop\n",
        "for epoch in range(args.num_epochs):\n",
        "    # initialize loss accumulator\n",
        "    epoch_loss = 0.\n",
        "    # do a training epoch over each mini-batch returned\n",
        "    # by the data loader\n",
        "    for batch_num, (faces, ratings, outcomes) in enumerate(face_outcome_emotion_loader):\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if args.cuda:\n",
        "            faces, ratings, outcomes = faces.cuda(), ratings.cuda(), outcomes.cuda()\n",
        "        \n",
        "        # do ELBO gradient and accumulate loss\n",
        "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
        "        epoch_loss += svi.step(images=faces, ratings=ratings, outcomes=outcomes)\n",
        "        epoch_loss += svi.step(images=faces, ratings=ratings, outcomes=None)\n",
        "        epoch_loss += svi.step(images=faces, ratings=None, outcomes=outcomes)\n",
        "        epoch_loss += svi.step(images=None, ratings=ratings, outcomes=outcomes)\n",
        "        epoch_loss += svi.step(images=faces, ratings=None, outcomes=None)\n",
        "        epoch_loss += svi.step(images=None, ratings=ratings, outcomes=None)\n",
        "        epoch_loss += svi.step(images=None, ratings=None, outcomes=outcomes)\n",
        "\n",
        "    # report training diagnostics\n",
        "    normalizer_train = len(face_outcome_emotion_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "    train_elbo.append(total_epoch_loss_train)\n",
        "    # report training diagnostics\n",
        "    trainingTimes.append(time.time())\n",
        "    epoch_time = trainingTimes[-1] - trainingTimes[-2]\n",
        "    print(\"[epoch %03d]  time: %.2f, average training loss: %.4f\" % (epoch, epoch_time, total_epoch_loss_train))\n",
        "    #if ((epoch+1) % 50 == 0):\n",
        "        #pyro.get_param_store().save('trained_models/checkpoints/tutorial_mvae_pretrained_' + str(epoch) + '.save')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Iaax9aSzW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_elbo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeR0RFwpWVCv",
        "colab_type": "text"
      },
      "source": [
        "It takes a while to train (far too slow for a laptop in a tutorial). Thus you can use the following chunks to save or load a model. We assume that you'll skip this save step and load the model from `trained_models/`. It's a large file so you can download it from: https://www.dropbox.com/s/ae157cladfneyxi/mvae_pretrained.save?dl=1 and put it into `trained_models/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olQCmfcPWeJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model if you decide to modify the above code to train your own model\n",
        "savemodel = False\n",
        "if savemodel:\n",
        "    if not os.path.exists('trained_models'):\n",
        "      os.mkdir('trained_models')\n",
        "    pyro.get_param_store().save('trained_models/mvae_pretrained.save')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVih5NRWlON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## making a \"trained_models\" directory\n",
        "if not os.path.exists('trained_models'):\n",
        "    os.mkdir('trained_models')\n",
        "\n",
        "## use this chunk to download a saved version of the model directly onto colab\n",
        "downloadmodel = True\n",
        "if downloadmodel:\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://www.dropbox.com/s/ae157cladfneyxi/mvae_pretrained.save?dl=1\", \n",
        "        \"trained_models/mvae_pretrained.save\")\n",
        "    print(\"Model downloaded\")\n",
        "\n",
        "## use this chunk to help upload the file if you have it on your local machine\n",
        "#from google.colab import files\n",
        "#!mkdir trained_models\n",
        "#%cd trained_models\n",
        "#files.upload()\n",
        "#%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgt3zx2pSzZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loadmodel = True\n",
        "if loadmodel:\n",
        "    pyro.get_param_store().load('trained_models/mvae_pretrained.save')\n",
        "    pyro.module(\"mvae\", mvae, update_module_params=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw6p0BhWWuuh",
        "colab_type": "text"
      },
      "source": [
        "Now, let's reconstruct some faces! We defined a function, `mvae.reconstruct_img_nosample()`, that allows us to condition on a given image and then reconstructe it. (Right now it's not yet fully optimized to allow sampling)\n",
        "\n",
        "- In the top row, we show 10 random samples from the training set.\n",
        "- In the bottom row, we show reconstructions from the faces in the top row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAowapHoSze1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_SAMPLES = 10\n",
        "input_array = np.zeros(shape=(IMG_WIDTH, 1, 3), dtype=\"uint8\")\n",
        "reconstructed_array = np.zeros(shape=(IMG_WIDTH, 1, 3), dtype=\"uint8\")\n",
        "\n",
        "for batch_num, (faces, ratings, outcomes) in enumerate(face_outcome_emotion_loader):\n",
        "    # pick NUM_SAMPLES random test images from the first mini-batch and\n",
        "    # visualize how well we're reconstructing them\n",
        "    if batch_num == 0:\n",
        "        reco_indices = np.random.randint(0, faces.size(0), NUM_SAMPLES)\n",
        "        for index in reco_indices:\n",
        "            input_img = faces[index, :]\n",
        "            # storing the input image\n",
        "            input_img_display = np.array(input_img*255., dtype='uint8')\n",
        "            input_img_display = input_img_display.transpose((1, 2, 0))\n",
        "            input_array = np.concatenate((input_array, input_img_display), axis=1)\n",
        "\n",
        "            # generating the reconstructed image and adding to array\n",
        "            input_img = input_img.view(1, 3, IMG_WIDTH, IMG_WIDTH)\n",
        "            reconstructed_img = mvae.reconstruct_img_nosample(input_img)\n",
        "            reconstructed_img = reconstructed_img.view(3, IMG_WIDTH, IMG_WIDTH).detach().numpy()\n",
        "            reconstructed_img = np.array(reconstructed_img*255., dtype='uint8')\n",
        "            reconstructed_img = reconstructed_img.transpose((1, 2, 0))\n",
        "            reconstructed_array = np.concatenate((reconstructed_array, reconstructed_img), axis=1)\n",
        "\n",
        "# remove first, blank column, and concatenate\n",
        "input_array = input_array[:,1:,:]\n",
        "reconstructed_array = reconstructed_array[:,1:,:]\n",
        "display_array = np.concatenate((input_array, reconstructed_array), axis=0)\n",
        "Image.fromarray(display_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsGtxzQZi_r",
        "colab_type": "text"
      },
      "source": [
        "It's not perfect -- there are still some artifacts in the reconstruction, which will take a bit of tweaking with the parameters (or more data) to fix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul9QrKV4W_yF",
        "colab_type": "text"
      },
      "source": [
        "## Coda\n",
        "\n",
        "In this tutorial, we leveraged a very recent published model (the MVAE from Wu & Goodman, 2018) to train a deep generative model that learns a latent (\"affect + face + outcome\") space from game outcomes, and their associated emotional faces and ratings.\n",
        "\n",
        "\n",
        "The model in this particular example is not completely optimized yet, because we are at the boundary of machine learning research, and we are also using a slightly more complicated dataset than say, MNIST or other common pedagogical dataset. We specifically chose an affective computing-relevant dataset so that we could outline psychological hypotheses and then, using probabilistic programming, build some quick models to test these.\n",
        "\n",
        "\n",
        "We hope that you have enjoyed this tutorial, and that it has been helpful!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjVxP0dTW_59",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "Written by: Desmond Ong (desmond.c.ong@gmail.com)\n",
        "\n",
        "References:\n",
        "\n",
        "Pyro [VAE tutorial](http://pyro.ai/examples/vae.html)\n",
        "\n",
        "M. Wu, & N. Goodman. (2018). Multimodal Generative Models for Scalable Weakly-Supervised Learning. In *Advances in Neural Information Processing Systems 31*. \n",
        "\n",
        "Hoffman, M. D., Blei, D. M., Wang, C., & Paisley, J. (2013). Stochastic\n",
        "variational inference. *The Journal of Machine Learning Research*, 14(1),\n",
        "1303-1347.\n",
        "\n",
        "Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. Auto-Encoding Variational Bayes. In *The International Conference on Learning Representations*. https://arxiv.org/abs/1312.6114\n",
        "\n",
        "Data from https://github.com/desmond-ong/affCog, from the following paper:\n",
        "\n",
        "Ong, D. C., Zaki, J., & Goodman, N. D. (2015). Affective Cognition: Exploring lay theories of emotion. *Cognition*, 143, 141-162."
      ]
    }
  ]
}