{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rational Speech Act framework\n",
    "(This tutorial written by Noah Goodman and Eli Bingham)\n",
    "\n",
    "Human language depends on the assumption of *cooperativity*, that speakers attempt to provide relevant information to the listener; listeners can use this assumption to reason *pragmatically* about the likely state of the world given the utterance chosen by the speaker.\n",
    "\n",
    "The Rational Speech Act framework formalizes these ideas using probabiistic decision making and reasoning.\n",
    "\n",
    "Note: This notebook must be run against Pyro 4392d54a220c328ee356600fb69f82166330d3d6 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first some imports\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)  # double precision for numerical stability\n",
    "\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "## this should work but doesn't, so i hacked it...:\n",
    "# from utils.search_inference import factor, HashingMarginal, memoize, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ngoodman/pplAffComp/code/utils\n"
     ]
    }
   ],
   "source": [
    "cd \"utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_inference import factor, HashingMarginal, memoize, Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can defined RSA, we specify a helper function that wraps up inference. `Marginal` takes an un-normalized stochastic function, constructs the distribution over execution traces by using `Search`, and constructs the marginal distribution on return values (via `HashingMarginal`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Marginal(fn):\n",
    "    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSA model captures recursive social reasoning -- a listener thinks about a speaker who thinks about a listener....\n",
    "\n",
    "To start, the `literal_listener` simply imposes that the utterance is true. Mathematically:\n",
    "$$P_\\text{Lit}(s|u) \\propto {\\mathcal L}(u,s)P(s)$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Marginal\n",
    "def literal_listener(utterance):\n",
    "    state = state_prior()\n",
    "    factor(\"literal_meaning\", 0. if meaning(utterance, state) else -999999.)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the cooperative speaker chooses an utterance to convey a given state to the literal listener. Mathematically:\n",
    "\n",
    "$$P_S(u|s) \\propto [P_\\text{Lit}(s|u) P(u)]^\\alpha$$\n",
    "\n",
    "In the code below, the `utterance_prior` captures the cost of producing an utterance, while the `pyro.sample` expression captures that the litteral listener guesses the right state (`obs=state` indicates that the sampled value is observed to be the correct `state`).\n",
    "\n",
    "We use `poutine.scale` to raise the entire execution probability to the power of `alpha` -- this yields a softmax decision rule with optimality parameter `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Marginal\n",
    "def speaker(state):\n",
    "    alpha = 1.\n",
    "    with poutine.scale(scale=torch.tensor(alpha)):\n",
    "        utterance = utterance_prior()\n",
    "        pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "    return utterance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define the pragmatic_listener, who infers which state is likely, given that the speaker chose a given utterance. Mathematically:\n",
    "\n",
    "$$P_L(s|u) \\propto P_S(u|s) P(s)$$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Marginal\n",
    "def pragmatic_listener(utterance):\n",
    "    state = state_prior()\n",
    "    pyro.sample(\"speaker\", speaker(state), obs=utterance)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a simple world by filling in the priors. We imagine there are 4 objects each either blue or red, and the possible utterances are \"none are blue\", \"some are blue\", \"all are blue\".\n",
    "\n",
    "We take the prior probabilities for the number of blue objects and the utterance to be uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number = 4\n",
    "\n",
    "def state_prior():\n",
    "    n = pyro.sample(\"state\", dist.Categorical(probs=torch.ones(total_number+1) / total_number+1))\n",
    "    return n\n",
    "\n",
    "def utterance_prior():\n",
    "    ix = pyro.sample(\"utt\", dist.Categorical(probs=torch.ones(3) / 3))\n",
    "    return [\"none\",\"some\",\"all\"][ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the meaning function (notated $\\mathcal L$ above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanings = {\n",
    "    \"none\": lambda N: N==0,\n",
    "    \"some\": lambda N: N>0,\n",
    "    \"all\": lambda N: N==total_number,\n",
    "}\n",
    "\n",
    "def meaning(utterance, state):\n",
    "    return meanings[utterance](state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it works: how does the pragmatic listener interpret the \"some\" utterance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0), 0.0), (tensor(1), 0.3125), (tensor(2), 0.3125), (tensor(3), 0.3125), (tensor(4), 0.0625)]\n"
     ]
    }
   ],
   "source": [
    "interp_dist = pragmatic_listener(\"some\")\n",
    "\n",
    "print([(s, interp_dist.log_prob(s).exp().item()) \n",
    "       for s in interp_dist.enumerate_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we get a *scalar implicature*: \"some\" is interpretted as likely not including all 4. Try looking at the `literal_listener` too -- no implicature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
