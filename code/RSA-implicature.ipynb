{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rational Speech Act framework\n",
    "\n",
    "Up here I will include some introduction to the RSA framework.\n",
    "\n",
    "Note: This notebook must be run against Pyro 4392d54a220c328ee356600fb69f82166330d3d6 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import collections\n",
    "import argparse\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "## this should work but doesn't, so i hacked it...:\n",
    "# from utils.search_inference import factor, HashingMarginal, memoize, Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'utils'\n",
      "/Users/ngoodman/pplAffComp/code/utils\n"
     ]
    }
   ],
   "source": [
    "cd \"utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_inference import factor, HashingMarginal, memoize, Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define helpers. \n",
    "\n",
    "`Marginal` takes an un-normalized stochastic function constructs the distribution over execution traces by using `Search`, and constructs the marginal distribution on return values (via `HashingMarginal`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)  # double precision for numerical stability\n",
    "\n",
    "def Marginal(fn):\n",
    "    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a simple world: there are 4 objects and utterances \"none are blue\", \"some are blue\", \"all are blue\".\n",
    "\n",
    "The first ingredients are priors: probabilities for the number of blue objects and the utterance. Both are taken to be uniform.\n",
    "\n",
    "Next are the meanings of the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number = 4\n",
    "\n",
    "def state_prior():\n",
    "    n = pyro.sample(\"state\", dist.Categorical(probs=torch.ones(total_number+1) / total_number+1))\n",
    "    return n\n",
    "\n",
    "def utterance_prior():\n",
    "    ix = pyro.sample(\"utt\", dist.Categorical(probs=torch.ones(3) / 3))\n",
    "    return [\"none\",\"some\",\"all\"][ix]\n",
    "\n",
    "meanings = {\n",
    "    \"none\": lambda N: N==0,\n",
    "    \"some\": lambda N: N>0,\n",
    "    \"all\": lambda N: N==total_number,\n",
    "}\n",
    "\n",
    "def meaning(utterance, state):\n",
    "    return meanings[utterance](state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the RSA model definitions.\n",
    "\n",
    "The `literal_listener` simply imposes that the utterance is true.\n",
    "\n",
    "The `speaker` chooses an utterance to convey `state` to the literal listener. (When `alpha` is greater than one the speaker is more optimal.)\n",
    "\n",
    "The `pragmatic_listener` infers which state is likely, given that the speaker chose a given utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Marginal\n",
    "def literal_listener(utterance):\n",
    "    state = state_prior()\n",
    "    factor(\"literal_meaning\", 0. if meaning(utterance, state) else -999999.)\n",
    "    return state\n",
    "\n",
    "\n",
    "@Marginal\n",
    "def speaker(state):\n",
    "    alpha = 1.\n",
    "    with poutine.scale(scale=torch.tensor(alpha)):\n",
    "        utterance = utterance_prior()\n",
    "        pyro.sample(\"listener\", literal_listener(utterance), obs=state)\n",
    "    return utterance\n",
    "\n",
    "\n",
    "@Marginal\n",
    "def pragmatic_listener(utterance):\n",
    "    state = state_prior()\n",
    "    pyro.sample(\"speaker\", speaker(state), obs=utterance)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it works: how does the pragmatic listener interpret the \"some\" utterance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(0), 0.0), (tensor(1), 0.3125), (tensor(2), 0.3125), (tensor(3), 0.3125), (tensor(4), 0.0625)]\n"
     ]
    }
   ],
   "source": [
    "interp_dist = pragmatic_listener(\"some\")\n",
    "\n",
    "print([(s, interp_dist.log_prob(s).exp().item()) \n",
    "       for s in interp_dist.enumerate_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we get a scalar implicature: \"some\" is interpretted as likely not including all 4. Try looking at the `literal_listener` too -- no implicature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
