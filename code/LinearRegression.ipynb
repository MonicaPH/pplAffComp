{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression as a model of Emotional Appraisal\n",
    "\n",
    "The first example we discuss involves building a simple linear regression model in Pyro. We consider a case that is relevant to affective computing --- how should we build a model to reason about someone's emotions? There are lots of emotion theories that one can draw from, and probabilistic programming offers an elegant way to specify (and test!) these theories.\n",
    "\n",
    "\n",
    "Before we jump in, we want to introduce the dataset that we'll be working with throughout the rest of the tutorial. This dataset is of a managable size (so training shouldn't take too long), but still complex enough that we can demonstrate some of the nice features of applying probabilistic programming to model real-life, AI-relevant examples (in this case, relevant to affective computing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will be using data from a published paper (Ong, Zaki, & Goodman, 2015; Experiment 3), which is available [here](https://github.com/desmond-ong/affCog) but we have also reproduced the data in the current repository. In this experiment, we showed human participants an agent playing a gamble; he spins a wheel with three possible outcomes, and wins the amount on the wheel. \n",
    "\n",
    "<div style=\"width: 500px; margin: auto;\">![Experiment Summary](images/experimentSummary.png)</div>\n",
    "(Figure modified from Ong et al, 2015, Figure 9).\n",
    "\n",
    "On some trials, participants see the outcome that the agent won ((i) above). On other trials ((ii) above), participants were not shown the outcome, but instead were shown what ostensibly was the agent's facial expression after seeing the outcome. And on the last third of trials ((iii) above), participants were shown both the outcome and the agent's facial expression.\n",
    "Following these, participants were asked to rate how they thought the agent felt, on 8 emotions, using a 9 point Likert scale.\n",
    "\n",
    "Thus, the dataset consists of some \"outcome only\" trials where participants saw outcomes and rated the agent's emotions, \"facial expression only\" trials where participants attributed emotions to a facial expression, and trials where they saw both and had to integrate the information from both the outcome and the facial expression to make a judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appraising the outcome\n",
    "\n",
    "Let us first consider the \"outcome only\" trials. Many established emotion theories and affective computing theories hold that people experiencing events (e.g., winning the lottery, missing the bus) will evaluate the situation according to a set of important features. Was the outcome desirable? Was the outcome surprising? Was the outcome controllable? This evaluation is known as **appraisal**.\n",
    "\n",
    "Put another way, the number of situations that people encounter in daily life vary immensely along a large number of dimensions, some important (the amount that one wins in the lottery) and some not so important (the color of the lottery ticket). **Appraisal** is computationally necessary to reduce the complexities of everyday situations into a low dimensional set of emotion-relevant dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Linear Regression model with Appraisal\n",
    "\n",
    "Thus, we already have the few basic ingreidents of our theory. We have an observable variable (the outcome). We have an appraisal process that converts the outcome into a small number of relevant features. And we have the emotion ratings that people produce. Let's construct a basic regression model:\n",
    "\n",
    "<div style=\"width: 300px; margin: auto;\">![Graphical Model](images/graphicalModel_LinearRegression.png)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to read the model above, which uses graphical model notation. Shaded circles represent observed variables, while unshaded represents latent, or unobserved variables. Small rectangles represent parameters (to be fitted). We have $N$ i.i.d. pairs of (*Outcome*, *Emotion Ratings*), where $N$ is the size of the dataset, and so these are represented in the large rectangle (called a \"plate\"), to indicate that they are repeated $N$ times. And between these *Outcome* and *Rating* pairs, we have an appraisal transformation.\n",
    "\n",
    "In a linear regression, we have $K+1$ regression weights that map the appraisal to the emotion ratings (the $+1$ is for the bias term). If the appraisal variables are given by $\\{1, a_1, a_2, \\ldots, a_K\\} = \\vec{a}$, we can write a regression equation: \n",
    "\n",
    "*Rating* = $\\vec{\\beta} \\cdot \\vec{a}$ = $\\beta_0$ + $\\beta_1 a_1$ + $\\beta_2 a_2$ + $\\ldots$ + $\\beta_K a_K$ + $\\epsilon$\n",
    "\n",
    "where the $\\beta_i$'s represent the regression weights for the $i$-th appraisal variable, $\\beta_0$ is the bias term and $\\epsilon$ is an error term. Notice that the $\\beta_i$'s should remain the same across all $N$ observations: thus, it is left out of the \"plate\" in the model diagram above.\n",
    "\n",
    "We assume that each $\\beta_i$ is drawn from a Normal distribution parameterized by a mean (location parameter) $\\mu_i$ and a standard deviation (or scale parameter) $\\sigma_i$, i.e., $\\beta_i \\sim N(\\mu_i, \\sigma_i)$. We wish to learn these parameters $\\mu_i, \\sigma_i$ from the data.\n",
    "\n",
    "\n",
    "(Note: *Appraisal* in this model is a strange creature. We could represent it as a latent variable (so like *outcome*, but unshaded). Here, to reflect the fact that *appraisal* is a modular function that can be tested scientifically against data, we chose to represent it more like a fittable parameter.).\n",
    "\n",
    "Let's write some Pyro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preamble\n",
    "\n",
    "This first chunk of code imports the necessary python packages and functions that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next chunk defines some variable and names that are specific to this dataset, as well as a function to read in the data.\n",
    "\n",
    "The data is stored in `outcome_emotion_dataset`, which is a torch Tensor of size (1541, 17), indicating that there are N=1,541 observations of 17 variables. The first 9 are the parameterization of the outcome (the 3 payoffs on the wheel and their probabilities, which outcome they won and that probability, and the angle within the sector that the wheel landed on), and the next 8 are the emotion variables. All the variables are scaled so that they lie within [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in dataset...\n",
      "Preview of first 3 rows:\n",
      "   payoff1  payoff2  payoff3  prob1  prob2  prob3  win  winProb  angleProp\n",
      "0     0.50     0.75      0.9   0.30   0.52   0.18  0.5     0.30      0.921\n",
      "1     0.15     0.70      0.8   0.45   0.29   0.26  0.8     0.26      0.873\n",
      "2     0.50     0.75      0.9   0.30   0.52   0.18  0.5     0.30      0.467\n",
      "   happy    sad  anger  surprise  disgust  fear  content  disapp\n",
      "0  0.625  0.000  0.000     0.625     0.25   0.0    0.625   0.375\n",
      "1  0.875  0.000  0.000     1.000     0.00   0.0    0.000   0.000\n",
      "2  0.625  0.125  0.125     0.250     0.00   0.0    0.250   0.500\n",
      "Shape of dataset:  torch.Size([1541, 17])\n"
     ]
    }
   ],
   "source": [
    "# data location\n",
    "dataset_path = os.path.join(os.path.abspath('..'), \"CognitionData\", \"data_wheelOnly.csv\")\n",
    "\n",
    "OUTCOME_VAR_NAMES = [\"payoff1\", \"payoff2\", \"payoff3\", \"prob1\", \"prob2\", \"prob3\", \"win\", \"winProb\", \"angleProp\"]\n",
    "EMOTION_VAR_NAMES = [\"happy\", \"sad\", \"anger\", \"surprise\", \"disgust\", \"fear\", \"content\", \"disapp\"]\n",
    "OUTCOME_VAR_DIM = len(OUTCOME_VAR_NAMES)\n",
    "EMOTION_VAR_DIM = len(EMOTION_VAR_NAMES)\n",
    "\n",
    "def load_outcome_emotion_dataset(csv_file, normalize_values=True, preview_datafile=False):\n",
    "    data_readin = pd.read_csv(csv_file)\n",
    "    outcome_data = data_readin.loc[:,OUTCOME_VAR_NAMES]\n",
    "    if normalize_values:\n",
    "        ####\n",
    "        ## payoff1, payoff2, payoff3 and win are between 0 and 100\n",
    "        ## need to normalize to [0,1] to match the rest of the variables,\n",
    "        ## by dividing payoff1, payoff2, payoff3 and win by 100\n",
    "        ####\n",
    "        outcome_data.loc[:,\"payoff1\"] = outcome_data.loc[:,\"payoff1\"]/100\n",
    "        outcome_data.loc[:,\"payoff2\"] = outcome_data.loc[:,\"payoff2\"]/100\n",
    "        outcome_data.loc[:,\"payoff3\"] = outcome_data.loc[:,\"payoff3\"]/100\n",
    "        outcome_data.loc[:,\"win\"]     = outcome_data.loc[:,\"win\"]/100\n",
    "    outcome_data_tensor = torch.tensor(outcome_data.values).type(torch.Tensor)\n",
    "    \n",
    "    # the actual data has 8 emotions, but for illustration we just use 1 emotion, happy\n",
    "    # the rest of the functions below assume a 1-D \"y\" variable\n",
    "    emotion_data = data_readin.loc[:,EMOTION_VAR_NAMES]\n",
    "    #emotion_data = data_readin.loc[:, \"happy\"]\n",
    "    if normalize_values:\n",
    "        ## note that emotions are transformed from a 9 point Likert to [0,1] via emo <- (emo-1)/8\n",
    "        emotion_data   = (emotion_data-1)/8\n",
    "    #emotion_data = emotion_data.values.reshape( emotion_data.shape[0] , 1)\n",
    "    #emotion_data = torch.tensor(emotion_data).type(torch.Tensor)\n",
    "    emotion_data_tensor = torch.tensor(emotion_data.values).type(torch.Tensor)\n",
    "    \n",
    "    if preview_datafile:\n",
    "        print(\"Preview of first 3 rows:\")\n",
    "        print(outcome_data.loc[0:2,:])\n",
    "        print(emotion_data.loc[0:2,:])\n",
    "    \n",
    "    data = torch.cat((outcome_data_tensor, emotion_data_tensor), 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# reads in datafile.\n",
    "print(\"Reading in dataset...\")\n",
    "outcome_emotion_dataset = load_outcome_emotion_dataset(csv_file=dataset_path, preview_datafile=True)\n",
    "N_samples = outcome_emotion_dataset.shape[0]\n",
    "\n",
    "print(\"Shape of dataset: \", outcome_emotion_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_appraisal()` is a function that takes in an outcome vector, and returns a vector of appraisal values. The example below reproduces the appraisal function used in Ong et al (2015). But more generally, this is a modular function that can be substituted out to test other possible operationalizations of appraisal theories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_appraisal(outcome_data):\n",
    "    # We have a simple hard-coded these appraisals, for illustration\n",
    "    # This is following Ong, Zaki, & Goodman (2015)\n",
    "    # the outcome data columns are, in order:\n",
    "    # [\"payoff1\", \"payoff2\", \"payoff3\", \"prob1\", \"prob2\", \"prob3\", \"win\", \"winProb\", \"angleProp\"]\n",
    "    # the 3 appraisal variables are: \n",
    "    #     amount won (\"win\"),\n",
    "    #     Prediction Error PE = win - EV, where EV = prob1*payoff1 + prob2*payoff2 + prob3*payoff3\n",
    "    #     absolute value of PE\n",
    "    \n",
    "    # if outcome_data only has 1 observation, reshape so vectorization works\n",
    "    if(len(outcome_data.shape)==1):\n",
    "        outcome_data = outcome_data.view(1,9)\n",
    "        print(outcome_data.shape)\n",
    "    \n",
    "    # initializing appraisalVals\n",
    "    appraisalVals = torch.zeros(size=(outcome_data.shape[0],3))\n",
    "    appraisalVals[:,0] = outcome_data[:,6] # amount won\n",
    "    \n",
    "    # Expected value\n",
    "    EV = outcome_data[:,0] * outcome_data[:,3] + \\\n",
    "         outcome_data[:,1] * outcome_data[:,4] + \\\n",
    "         outcome_data[:,2] * outcome_data[:,5]\n",
    "    \n",
    "    # prediction error and absolute PE\n",
    "    appraisalVals[:,1] = appraisalVals[:,0] - EV\n",
    "    appraisalVals[:,2] = abs(appraisalVals[:,1])\n",
    "    return(appraisalVals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model\n",
    "\n",
    "Next, we have the model. Let's break down what goes on.\n",
    "\n",
    "First, the model samples some $\\beta$ coefficients from a Normal with some priors over the mean and the scale (in this case, mean of 0 and scale of 1).\n",
    "This is achieved using the `pyro.sample()` function. \n",
    "For example,\n",
    "\n",
    "`b_0 = pyro.sample(\"b_0\", Normal(coeff_mean_prior, coeff_scale_prior))`\n",
    "\n",
    "\n",
    "Note that the `sample()` function takes in a variable name, which allows Pyro to uniquely identify that variable in its variable store. (As such, the variable names are unique, and you can only have one `sample()` function with a particular variable name in this function).\n",
    "\n",
    "\n",
    "\n",
    "Next, the function will loop over the observed data, using `pyro.iarange()`. This function defines a special Pyro environment with a unique name (`\"map\"`), within which Pyro understands that each iteration of the \"loop\" is conditionally independent. Thus, the computation on each data-point is conditionally independent from the computation on other data-points. (This reflects the plate-notation in the model above; each datapoint is independent, BUT the $\\beta$ coefficients are the same across all of them, that's why they were defined before the `pyro.iarange()` loop)\n",
    "\n",
    "Within this loop, we take the `outcome_data`, run it through `compute_appraisal()` to get a small 3-dimensional `appraisal_vars`. We manually compute the regression equation:\n",
    "\n",
    "`prediction = b_0 + b_1 * appraisal_vars[:,0] + b_2 * appraisal_vars[:,1] + b_3 * appraisal_vars[:,2]`\n",
    "\n",
    "Thus, `prediction` is the mean of the Normal distribution that the linear regression model predicts.\n",
    "Finally, we condition on the observed data:\n",
    "\n",
    "`pyro.sample(\"obs\", Normal(prediction, 1), obs = emotion_data)`\n",
    "\n",
    "Notice we use `pyro.sample()` again. We draw a sample from a Normal with mean `prediction` and scale 1, but this time, we condition that this sample is equal to the observed `emotion_data`, using the argument `obs = ...`.\n",
    "\n",
    "\n",
    "And that's basically it for this function. Pyro's `irange()` and `iarange()` [functions](http://pyro.ai/examples/svi_part_ii.html#iarange) allow a flexible way to perform computations on individual datapoints while taking care of conditional independencies (the `i` in `irange()` and `iarange()`). One difference is that `iarange()` is vectorized, so we can perform the calculations on the entire data Tensor instead of individual observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression_model(data):\n",
    "    # define the parameters that control the gaussian prior over the regression coeffs.\n",
    "    # mean = 0, scale = 1\n",
    "    coeff_mean_prior = torch.tensor(0.0)\n",
    "    coeff_scale_prior = torch.tensor(1.0)\n",
    "    \n",
    "    # sample b_0 (intercept) and b_1 to b_3 (regression coeffs)\n",
    "    b_0 = pyro.sample(\"b_0\", Normal(coeff_mean_prior, coeff_scale_prior))\n",
    "    b_1 = pyro.sample(\"b_1\", Normal(coeff_mean_prior, coeff_scale_prior))\n",
    "    b_2 = pyro.sample(\"b_2\", Normal(coeff_mean_prior, coeff_scale_prior))\n",
    "    b_3 = pyro.sample(\"b_3\", Normal(coeff_mean_prior, coeff_scale_prior))\n",
    "    \n",
    "    # loop over observed data\n",
    "    with pyro.iarange(\"map\", data.shape[0]):\n",
    "        outcome_data = data[:, :(OUTCOME_VAR_DIM)]\n",
    "        # Here, for simplification, we are only taking one emotion variable (happy)\n",
    "        # instead of all 8 emotions\n",
    "        emotion_data = data[:, OUTCOME_VAR_DIM]  \n",
    "        appraisal_vars = compute_appraisal(outcome_data)\n",
    "        \n",
    "        # run the regression forward\n",
    "        prediction = b_0 + b_1 * appraisal_vars[:,0] + b_2 * appraisal_vars[:,1] + b_3 * appraisal_vars[:,2]\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\", Normal(prediction, 1), obs = emotion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guide\n",
    "\n",
    "todo: talk about guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression_guide(data):\n",
    "    mean_b0_param  = pyro.param(\"guide_mean_b0\",  torch.tensor(0.0))\n",
    "    scale_b0_param = pyro.param(\"guide_scale_b0\", torch.tensor(1.0))\n",
    "    mean_b1_param  = pyro.param(\"guide_mean_b1\",  torch.tensor(0.0))\n",
    "    scale_b1_param = pyro.param(\"guide_scale_b1\", torch.tensor(1.0))\n",
    "    mean_b2_param  = pyro.param(\"guide_mean_b2\",  torch.tensor(0.0))\n",
    "    scale_b2_param = pyro.param(\"guide_scale_b2\", torch.tensor(1.0))\n",
    "    mean_b3_param  = pyro.param(\"guide_mean_b3\",  torch.tensor(0.0))\n",
    "    scale_b3_param = pyro.param(\"guide_scale_b3\", torch.tensor(1.0))\n",
    "    # sample coefficients from Normal(mean, scale)\n",
    "    pyro.sample(\"b_0\", Normal(mean_b0_param, scale_b0_param))\n",
    "    pyro.sample(\"b_1\", Normal(mean_b1_param, scale_b1_param))\n",
    "    pyro.sample(\"b_2\", Normal(mean_b2_param, scale_b2_param))\n",
    "    pyro.sample(\"b_3\", Normal(mean_b3_param, scale_b3_param))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the model\n",
    "\n",
    "Next, we can proceed to actually fit the model. The first step is to refresh the parameter store using `pyro.clear_param_store()`.\n",
    "\n",
    "We will use Stochastic Variational Inference `SVI()` which takes in the model and guide that we wrote above, as well as an optimization algorithm (here we use `torch.optim.Adam()`) and a loss function (here we use `Trace_ELBO()`). When `svi.step(data)` is called, it runs SVI over the `data`. Thus, here we simply define a loop that runs over the entire dataset `num_iterations` times. (We can easily modify this to do mini-batching, for example, for large datasets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 2.4724\n",
      "[iteration 0101] loss: 1.9535\n",
      "[iteration 0201] loss: 1.0219\n",
      "[iteration 0301] loss: 1.1010\n",
      "[iteration 0401] loss: 0.9693\n",
      "[iteration 0501] loss: 0.9467\n",
      "[iteration 0601] loss: 0.9617\n",
      "[iteration 0701] loss: 0.9452\n",
      "[iteration 0801] loss: 0.9524\n",
      "[iteration 0901] loss: 0.9431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'loss')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "# setup the optimizer with some learning rate\n",
    "optimizer = Adam({\"lr\": 0.005})\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(fit_regression_model, fit_regression_guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "losses = []\n",
    "for thisIteration in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    thisLoss = svi.step(outcome_emotion_dataset)\n",
    "    losses.append(thisLoss)\n",
    "    if thisIteration % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (thisIteration + 1, thisLoss / float(N_samples)))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cannot initialize a parameter 'scale_b0_param' with None. Did you get the param name right?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-037994c75555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# output the learned variational parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b0 ~ Normal(%.4f, %.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guide_mean_b0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale_b0_param\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b1 ~ Normal(%.4f, %.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guide_mean_b1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale_b1_param\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b2 ~ Normal(%.4f, %.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guide_mean_b2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale_b2_param\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b3 ~ Normal(%.4f, %.4f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guide_mean_b3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scale_b3_param\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/desmond/git/pplAffComp/env/lib/python2.7/site-packages/pyro/primitives.pyc\u001b[0m in \u001b[0;36mparam\u001b[0;34m(name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mam_i_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_PYRO_PARAM_STORE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         msg = {\n",
      "\u001b[0;32m/Users/desmond/git/pplAffComp/env/lib/python2.7/site-packages/pyro/params/param_store.pyc\u001b[0m in \u001b[0;36mget_param\u001b[0;34m(self, name, init_tensor, constraint)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# if not create the init tensor through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0minit_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0;34m\"cannot initialize a parameter '{}' with None. Did you get the param name right?\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: cannot initialize a parameter 'scale_b0_param' with None. Did you get the param name right?"
     ]
    }
   ],
   "source": [
    "# output the learned variational parameters\n",
    "print(\"b0 ~ Normal(%.4f, %.4f)\" % (pyro.param(\"guide_mean_b0\").item(), pyro.param(\"scale_b0_param\").item()))\n",
    "print(\"b1 ~ Normal(%.4f, %.4f)\" % (pyro.param(\"guide_mean_b1\").item(), pyro.param(\"scale_b1_param\").item()))\n",
    "print(\"b2 ~ Normal(%.4f, %.4f)\" % (pyro.param(\"guide_mean_b2\").item(), pyro.param(\"scale_b2_param\").item()))\n",
    "print(\"b3 ~ Normal(%.4f, %.4f)\" % (pyro.param(\"guide_mean_b3\").item(), pyro.param(\"scale_b3_param\").item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coda\n",
    "\n",
    "Hopefully the example above illustrates several appealing features of probabilistic programming:\n",
    "\n",
    "- We can use PPLs to specify theory, and importantly, uncertainty in theory and random processes.\n",
    "- Theory is represented as modular chunks of code. (As in our `compute_appraisal()` function, or in the linear regression model.) This allows us to substitute out different parts of the model, which is handy for optimization or theory testing!\n",
    "    - Want to try a different appraisal representation? No problem! \n",
    "    - Want to try using a feed-forward neural network instead of a linear regression? Sure (and in fact we will cover that in the next example!)\n",
    "- Inference and learning is orthogonal to model specification. Thus, the modeler can focus on specifying the model, while Pyro does most of the heavy lifting (by leveraging PyTorch modules).\n",
    "\n",
    "\n",
    "In the next example, we show how to incorporate a feed-forward neural network in order to build a deep generative model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra: Some further extensions to the linear regression model\n",
    "\n",
    "In the following code we modify the earlier model to handle an arbitrary number of regression parameters, and introduce the notion of \"lifting\" models to get distributions over *models* that we can sample from.\n",
    "\n",
    "Instead of using a low-dimensional `compute_appraisal()` function, we demonstrate a standard linear regression by defining an `appraisalRegressionModule()` that inherits from PyTorch's `nn.module`. (Note that a linear regression can be thought of as a feed-forward neural network with 0 hidden layers and no non-linearities, such that the output layer is just a linear combination of the input units). This is achieved using `nn.Linear()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class appraisalRegressionModule(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(appraisalRegressionModule, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, outcome):\n",
    "        return self.linear(outcome)\n",
    "\n",
    "regression_model = appraisalRegressionModule(OUTCOME_VAR_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the model. The main difference now is that we can define separate location and scale priors for each regressor, given by `weights_loc` and `weights_scale` below, which are each Tensors of Size(1,`OUTCOME_VAR_DIM`) (or any arbitrary length given by an appraisal transformation).\n",
    "\n",
    "Rather than typing out `OUTCOME_VAR_DIM` `pyro.sample()` statements, a fancier way of sampling using `weights_loc, weights_scale` is to \"lift\" the regression model using `pyro.random_module()`. This takes a model and a prior, and returns a *distribution over possible models*. In other words, we can think of this step as sampling models from a \"model-distribution\" (which under the hood, Pyro does by sampling the regression weights from their priors). Note that we do not see any `pyro.sample()` calls in the code below, as compared to the earlier code: these are taken care of by Pyro and will be executed when sampling from the distribution returned by `random_module()`.\n",
    "\n",
    "The rest of the code is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesianRegressionModel(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    weights_loc   = torch.zeros(size=(torch.Size((1, OUTCOME_VAR_DIM))))\n",
    "    weights_scale = torch.ones(size=(torch.Size((1, OUTCOME_VAR_DIM))))\n",
    "    weights_prior = Normal(weights_loc, weights_scale).independent(1)\n",
    "    \n",
    "    # location and scale prior for the bias\n",
    "    bias_loc   = torch.zeros(size=(torch.Size((1, ))))\n",
    "    bias_scale = torch.ones(size=(torch.Size((1, ))))\n",
    "    bias_prior = Normal(bias_loc, bias_scale).independent(1)\n",
    "    \n",
    "    priors = {'linear.weight': weights_prior, 'linear.bias': bias_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a model (which also samples from weights_prior and bias_prior)\n",
    "    sampled_regression_model = lifted_module()\n",
    "    \n",
    "    with pyro.iarange(\"map\", data.shape[0]):\n",
    "        outcome_data = data[:, :(OUTCOME_VAR_DIM)]\n",
    "        # Here, for simplification, we are only taking one emotion variable (happy)\n",
    "        # instead of all 8 emotions\n",
    "        emotion_data = data[:, OUTCOME_VAR_DIM]  \n",
    "        \n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction = sampled_regression_model(outcome_data).squeeze(-1)\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\", Normal(prediction, 1), obs = emotion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the guide function in a parallel manner to the model function. Note that we register the variational parameters in Pyro's parameter store using the `pyro.param()` function, just like above. We do the same trick of lifting the regression model using a `pyro.random_module()` call and sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesianRegressionGuide(data):\n",
    "    # define our variational parameters\n",
    "    weights_loc   = torch.randn(1, OUTCOME_VAR_DIM)\n",
    "    # Note that the scale has to be non-negative. Thus, we use exp() to get a non-negative number.\n",
    "    # we also use a narrower scale (exp(-1) ~ 0.35 instead of exp(0) = 1)\n",
    "    weights_scale = torch.exp(-1.0 * torch.ones(1, OUTCOME_VAR_DIM) + 0.05 * torch.randn(1, OUTCOME_VAR_DIM))\n",
    "    bias_loc      = torch.randn(1)\n",
    "    bias_scale    = torch.exp(-1.0 * torch.ones(1) + 0.05 * torch.randn(1))\n",
    "\n",
    "    # using pyro.param() to register the variational parameters\n",
    "    weight_loc_param   = pyro.param(\"guide_loc_weight\", weights_loc)\n",
    "    weight_scale_param = pyro.param(\"guide_scale_weight\", weights_scale)\n",
    "    bias_loc_param     = pyro.param(\"guide_loc_bias\", bias_loc)\n",
    "    bias_scale_param   = pyro.param(\"guide_scale_bias\", bias_scale)\n",
    "    # guide distributions for w and b\n",
    "    weight_dist = Normal(weight_loc_param, weight_scale_param).independent(1)\n",
    "    bias_dist   = Normal(bias_loc_param, bias_scale_param).independent(1)\n",
    "    dists = {'linear.weight': weight_dist, 'linear.bias': bias_dist}\n",
    "    # lift the module and sample from that distribution\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit this regression model using SVI; this is identical to the code earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "# setup the optimizer with some learning rate\n",
    "optimizer = Adam({\"lr\": 0.005})\n",
    "\n",
    "# setup the inference algorithm\n",
    "bayesianRegressionSVI = SVI(bayesianRegressionModel, bayesianRegressionGuide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "losses = []\n",
    "for thisIteration in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    thisLoss = bayesianRegressionSVI.step(outcome_emotion_dataset)\n",
    "    losses.append(thisLoss)\n",
    "    if thisIteration % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (thisIteration + 1, thisLoss / float(N_samples)))\n",
    "        \n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name in pyro.get_param_store().get_all_param_names():\n",
    "#    print(name, pyro.param(name).data.numpy())\n",
    "\n",
    "guide_loc_weight = pyro.param(\"guide_loc_weight\")[0]\n",
    "guide_scale_weight = pyro.param(\"guide_scale_weight\")[0]\n",
    "\n",
    "print(\"b0 ~ Normal(%.4f, %.4f)\" % (pyro.param(\"guide_loc_bias\").item(), pyro.param(\"guide_scale_bias\").item()))\n",
    "for j in xrange(len(guide_loc_weight)):\n",
    "    print(\"b%1d\" % (j+1), \"~ Normal(%.4f, %.4f)\" % (guide_loc_weight[j], guide_scale_weight[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
