{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to read faces.\n",
    "\n",
    "In the previous example, we saw how to hand-specify a \"theory\" of how people feel in response to outcomes. But it is impossible to hand-specify other processes, such as how emotions translate into facial expressions. We can posit a theory that feeling an emotion may 'cause' someone to display some facial expressions (e.g. *happy* 'causes' smiling, *anger* 'causes' a furrowing of the eyebrows), but it may not be practical to spell out the possible sets of muscular configurations---as well as variance in these configurations---that accompany each emotion. Thus, from a computational standpoint, we also have to be able to learn how to \"read\" unstructured data like images of faces (or audio, or video, or text).\n",
    "\n",
    "In this example, we extend the model from the previous example to demonstrate how we can learn to \"read\" emotions from faces using deep neural networks, but within a probabilistic generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following graphical model:\n",
    "\n",
    "<div style=\"width: 300px; margin: auto;\">![Graphical Model](images/graphicalModel_SSVAE.png)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For simplicity, we have left out the other parameters in the \"appraisal\" part of the model)\n",
    "\n",
    "We have added an observed variable, **Facial Expressions**, which are 'caused' by **Emotion Ratings**. The transformation from Ratings to Expressions, $P_{\\theta}(\\text{face }|\\text{ ratings})$ is parameterized by $\\theta$. But the face contains many other aspects that are not determined by the emotion, such as the face shape, gender, and race, and it would be ideal to try to model those aspects separately from the face. One way to do this is to add an additional latent variable, $z$, to capture these non-emotional features, and then try to learn a model $P_{\\theta}(\\text{face }| \\text{ rating}, z)$.\n",
    "\n",
    "\n",
    "The parameters $\\theta$ can be learnt using a technique called stochastic variational inference (SVI), which we saw briefly in the last example as well. \n",
    "\n",
    "\n",
    "If we just had the model $z \\rightarrow \\text{face}$, which is a deep generative model with a latent cause, then we can fit this using a technique called variational inference: this would be an example of a variational autoencoder (VAE; Kingma & Welling, 2014).\n",
    "\n",
    "The pa- rameters ùúÉ can be learnt via stochastic variational infer- ence (SVI) [32]. Modern probabilistic programming lan- guages are able to perform SVI automatically with a small amount of input from the modeler. SVI historically re- quired the derivation of a quantity called the evidence lower bound (ELBO)‚Äîthe ELBO is maximized during training, much like how a loss function is minimized dur- ing many machine learning approaches. In practice, the ELBO contains the posterior distribution (e.g. P(z | face)), which is often intractable, but can be approximated with variational distributions (in our case, q(z | face)) that can also be parameterized by neural networks. Trained in this manner, the model is a semi-supervised variant of the variational autoencoder (VAE) [39], a popular generative model that has received significant attention in the deep learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "We will be using the same dataset as the previous example. Now, we will consider the trials in which participants only saw a facial expression, and rated how they thought the character feels, or what we call the \"facial expression only\" trials.\n",
    "\n",
    "Here is an example face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "from visdom import Visdom\n",
    "\n",
    "#from utils.vae_plots import plot_llk, plot_vae_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import ImageMath \n",
    "from PIL import Image\n",
    "\n",
    "from pyro.contrib.examples.util import print_and_log, set_seed\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "from utils.custom_mlp import MLP, Exp\n",
    "from utils.mnist_cached import  mkdir_p, setup_data_loaders\n",
    "#from utils.vae_plots import plot_conditional_samples_ssvae, plot_vae_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display imagefile\n",
    "facedatadir = os.path.join(os.path.abspath('..'), \"CognitionData\", \"more_faces\", \"small\")\n",
    "t1 = ToTensor(io.imread(os.path.join(facedatadir, \"imgs\", \"f_0.png\")))\n",
    "#t1 = ToTensor(io.imread(os.path.join(datadir, \"imgs\", \"face_3_0_1.png\")))\n",
    "ToPILImage(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiSupervisedVAE():\n",
    "    def condition(self, outcome, emotion, image):\n",
    "        # generate a new emotion from outcome\n",
    "        prediction_mean = self.outcomesToEmotion(outcomes)     \n",
    "        # condition on the observed data\n",
    "        emo = pyro.sample(\"emo\", Normal(prediction_mean, 1), obs=emotion)\n",
    "        # sample z given priors\n",
    "        z = pyro.sample(\"z\", dist.Normal(prior_location, prior_scale))\n",
    "        # generate the face using emotion and z\n",
    "        # conditioned on observed image\n",
    "        zAndEmo = torch.cat((z, emo), 1)\n",
    "        loc = self.zAndEmosToFace_Decoder(zAndEmo)           \n",
    "        pyro.sample(\"face\", dist.Bernoulli(loc), obs=image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can separate the auxillary code / helper functions into a separate .py file that is imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
