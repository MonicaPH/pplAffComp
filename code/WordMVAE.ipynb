{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions to a multimodal VAE\n",
    "\n",
    "Another way this model can be extended is to fully let the latent $z$ \"cause\" both the emotion ratings and the facial expression. That's an example of a Multimodal VAE (Wu & Goodman, 2018).\n",
    "\n",
    "There is a nice theoretical motivation for this model too. Throughout, we've assumed that the space of emotions is exactly what we measured (e.g., some value of happiness, some value of sadness), but maybe the latent space is more structured, but not along these discrete emotion categories -- perhaps along dimensions like \"good\" vs \"bad\", or . In emotion theory, this undifferentiated space is called affect, and often, this is a low-dimensional space (2 to 3 dimensions capture most of the variance in empirical data).\n",
    "\n",
    "We could thus posit a latent *affect*, and actually we would still want a $z$ that captures non-emotional aspects of the face -- learning to disentangle latent variables is also an active area of research (Narayanaswamy et al, 2017).\n",
    "\n",
    "And finally, we can add the \"outcome to appraisal to affect\" part back into this multimodal model.\n",
    "\n",
    "<div style=\"width: 300px; margin: auto; \">![Graphical Model](images/graphicalModel_MVAE.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import division, print_function, absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.transforms import ToPILImage\n",
    "from skimage import io, transform\n",
    "from scipy.special import expit\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from pyro.contrib.examples.util import print_and_log, set_seed\n",
    "import pyro.poutine as poutine\n",
    "# custom helperCode for this tutorial, in helperCode.py\n",
    "import helperCode\n",
    "from utils.custom_mlp import MLP, Exp\n",
    "\n",
    "\n",
    "from visdom import Visdom\n",
    "\n",
    "#from utils.vae_plots import plot_llk, plot_vae_samples\n",
    "from utils.mnist_cached import  mkdir_p, setup_data_loaders\n",
    "from utils.vae_plots import plot_conditional_samples_ssvae, plot_vae_samples\n",
    "\n",
    "EMBED_DIM = 50\n",
    "IMG_WIDTH = 64\n",
    "IMG_SIZE = IMG_WIDTH*IMG_WIDTH*3\n",
    "BATCH_SIZE = 32\n",
    "DEFAULT_HIDDEN_DIMS = [200,200] #[500, 500]\n",
    "DEFAULT_Z_DIM = 25#50#2\n",
    "\n",
    "# FACE_VAR_NAMES = ['facePath']\n",
    "OUTCOME_VAR_NAMES = ['payoff1', 'payoff2', 'payoff3', \n",
    "                     'prob1', 'prob2', 'prob3', \n",
    "                     'win', 'winProb', 'angleProp']\n",
    "EMOTION_VAR_NAMES = ['happy', 'sad', 'anger', 'surprise', \n",
    "                     'disgust', 'fear', 'content', 'disapp']\n",
    "\n",
    "OUTCOME_VAR_DIM = len(OUTCOME_VAR_NAMES)\n",
    "EMOTION_VAR_DIM = len(EMOTION_VAR_NAMES)\n",
    "\n",
    "OUTCOME_VAR_DIM_COLLAPSE = len(OUTCOME_VAR_NAMES) - 2 + 3\n",
    "EMOTION_VAR_DIM_COLLAPSE = len(EMOTION_VAR_NAMES) * 9\n",
    "\n",
    "def swish(x):\n",
    "    return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings\n",
    "\n",
    "First we define some helper functions for comparing word similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.sqrt(v.dot(v))\n",
    "    return v / norm\n",
    "\n",
    "def cosine_sim_np(a, b):\n",
    "    if normalize_embeddings:\n",
    "        return np.dot(a, b)\n",
    "    else:\n",
    "        return np.dot(normalize(a), normalize(b))\n",
    "\n",
    "def cosine_sim_torch(a, b):\n",
    "    a_norm = a / a.norm()\n",
    "    b_norm = b / b.norm()    \n",
    "    return torch.dot(a_norm, b_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the GloVe word embeddings. (Warning: may take up to a minute...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = os.path.join(os.path.abspath('..'), \"glove\", \"glove.6B.50d.txt\")\n",
    "\n",
    "# Whether or not to normalize the word vectors\n",
    "normalize_embeddings = False\n",
    "    \n",
    "def load_glove_embeddings(path):\n",
    "    print(\"Loading GloVe embeddings\")\n",
    "    with open(path,'r') as f:\n",
    "        model = {}\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]], dtype=np.float32)\n",
    "            if normalize_embeddings:\n",
    "                embedding = normalize(embedding)\n",
    "            model[word] = embedding\n",
    "        print(\"Done.\",len(model),\" words loaded!\")\n",
    "        return model\n",
    "\n",
    "embeddings = load_glove_embeddings(embed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "Here we define a class to load multimodal data (word embeddings, faces, emotion ratings, and outcomes), allowing for missing values (which are set to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"A multimodal experimental dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, embeddings=None, img_dir=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the experiment csv file \n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.expdata = pd.read_csv(csv_file)\n",
    "        self.embeddings = embeddings\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.has_utterances = False\n",
    "        self.has_faces = False\n",
    "        self.has_emotions = False\n",
    "        self.has_outcomes = False\n",
    "        \n",
    "        # Check if dataset has utterances\n",
    "        if \"utterance\" in self.expdata.columns and embeddings is not None:\n",
    "            self.has_utterances = True\n",
    "        # Check if dataset has face images\n",
    "        if \"facePath\" in self.expdata.columns and img_dir is not None:\n",
    "            self.has_faces = True\n",
    "        # Check if dataset has emotion ratings\n",
    "        if set(EMOTION_VAR_NAMES).issubset(self.expdata.columns):\n",
    "            self.has_emotions = True\n",
    "            self.normalize_emotions()\n",
    "        # Check if dataset has outcomes\n",
    "        if set(OUTCOME_VAR_NAMES).issubset(self.expdata.columns):\n",
    "            self.has_outcomes = True\n",
    "            self.normalize_outcomes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expdata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.has_emotions:\n",
    "            emotions = np.array(self.expdata.iloc[idx][\"happy\":\"disapp\"], np.float32)\n",
    "        else:\n",
    "            emotions = 0\n",
    "\n",
    "        if self.has_outcomes:\n",
    "            outcomes = np.array(self.expdata.iloc[idx][\"payoff1\":\"angleProp\"], np.float32)\n",
    "        else:\n",
    "            outcomes = 0\n",
    "\n",
    "        if self.has_utterances:\n",
    "            word = self.expdata.iloc[idx][\"utterance\"]\n",
    "            embed = self.embeddings[word]\n",
    "        else:\n",
    "            word = \"\"\n",
    "            embed = 0\n",
    "        \n",
    "        if self.has_faces:\n",
    "            img_name = os.path.join(self.img_dir, self.expdata.iloc[idx][\"facePath\"] + \".png\")\n",
    "            try:\n",
    "                image = Image.open(img_name).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "            except:\n",
    "                print(img_name)\n",
    "                raise\n",
    "        else:\n",
    "            image = 0\n",
    "            \n",
    "        return word, embed, image, emotions, outcomes\n",
    "    \n",
    "    def normalize_outcomes(self):\n",
    "        \"\"\"Normalizes outcome data.\n",
    "        \n",
    "        payoff1, payoff2, payoff3 and win are between 0 and 100\n",
    "        need to normalize to [0,1] to match the rest of the variables,\n",
    "        by dividing payoff1, payoff2, payoff3 and win by 100.\n",
    "        \"\"\"        \n",
    "        self.expdata.loc[:,\"payoff1\"] = self.expdata.loc[:,\"payoff1\"]/100\n",
    "        self.expdata.loc[:,\"payoff2\"] = self.expdata.loc[:,\"payoff2\"]/100\n",
    "        self.expdata.loc[:,\"payoff3\"] = self.expdata.loc[:,\"payoff3\"]/100\n",
    "        self.expdata.loc[:,\"win\"]     = self.expdata.loc[:,\"win\"]/100\n",
    "    \n",
    "    def normalize_emotions(self):\n",
    "        \"\"\"Normalize emotion ratings.\n",
    "        \n",
    "        Emotions were rated on a 1-9 Likert scale.\n",
    "        use emo <- (emo-1)/8 to transform to within [0,1]\n",
    "        \"\"\"\n",
    "        self.expdata.loc[:,\"happy\":\"disapp\"] = (self.expdata.loc[:,\"happy\":\"disapp\"]-1)/8\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and store the face/outcome/emotion data in `face_outcome_emotion_dataset`. There are N=1,587 observations, and each observation consists of:\n",
    "\n",
    "- an accompanying face image\n",
    "- a 9-dimension outcome vector that parameterizes the gamble that agents played, and\n",
    "- an 8-dimensional emotion rating vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    # Note that we downsample to 64 x 64 here, because we wanted a nice power of 2 \n",
    "    #(and DCGAN architecture assumes input image of 64x64) \n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# data location\n",
    "faces_path = os.path.join(os.path.abspath('..'), \"CognitionData\", \"faces\")\n",
    "face_outcome_emotion_path = os.path.join(os.path.abspath('..'), \"CognitionData\", \"data_faceWheel.csv\")\n",
    "\n",
    "# reads in datafile.\n",
    "print(\"Reading in dataset...\")\n",
    "\n",
    "face_outcome_emotion_dataset = MultimodalDataset(csv_file=face_outcome_emotion_path, \n",
    "                                                 img_dir=faces_path, \n",
    "                                                 transform=img_transform)\n",
    "face_outcome_emotion_loader = torch.utils.data.DataLoader(face_outcome_emotion_dataset,\n",
    "                                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                          num_workers=4)\n",
    "\n",
    "N_samples = len(face_outcome_emotion_dataset)\n",
    "print(\"Number of observations:\", N_samples)\n",
    "\n",
    "# taking a sample observation\n",
    "word, embed, img, emo, out = face_outcome_emotion_dataset[np.random.randint(0, N_samples)]\n",
    "print(\"Sample Observation: \")\n",
    "print(\"Ratings:\")\n",
    "row_fmt =\"{:<8} \" * len(emo)\n",
    "print(row_fmt.format(*helperCode.EMOTION_VAR_NAMES))\n",
    "row_fmt =\"{:<8.3f} \" * len(emo)\n",
    "print(row_fmt.format(*emo))\n",
    "print(\"Outcomes:\")\n",
    "row_fmt =\"{:<8} \" * len(out)\n",
    "print(row_fmt.format(*helperCode.OUTCOME_VAR_NAMES))\n",
    "row_fmt =\"{:<8.3f} \" * len(out)\n",
    "print(row_fmt.format(*out))\n",
    "Image.fromarray(helperCode.TensorToPILImage(img*255.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall also load the dataset of utterances, outcomes and emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "word_outcome_emotion_path = os.path.join(os.path.abspath(\"..\"), \"CognitionData\", \"dataSecondExpt_utteranceWheel.csv\")\n",
    "expdata = pd.read_csv(word_outcome_emotion_path)\n",
    "\n",
    "# Print utterances\n",
    "utterances = list(sorted(pd.unique(expdata.loc[:][\"utterance\"])))\n",
    "print(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in datafile.\n",
    "print(\"Reading in dataset...\")\n",
    "\n",
    "word_outcome_emotion_dataset = MultimodalDataset(csv_file=word_outcome_emotion_path, \n",
    "                                                 embeddings=embeddings)\n",
    "word_outcome_emotion_loader = torch.utils.data.DataLoader(word_outcome_emotion_dataset,\n",
    "                                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                          num_workers=4)\n",
    "\n",
    "N_samples = len(word_outcome_emotion_dataset)\n",
    "print(\"Number of observations:\", N_samples)\n",
    "\n",
    "# Taking a sample observation\n",
    "word, embed, img, emo, out = word_outcome_emotion_dataset[np.random.randint(0, N_samples)]\n",
    "print(\"Sample Observation: \")\n",
    "print(\"Utterance:\", word)\n",
    "print(\"Embedding:\")\n",
    "print(embed)\n",
    "print(\"Ratings:\")\n",
    "row_fmt =\"{:<8} \" * len(emo)\n",
    "print(row_fmt.format(*helperCode.EMOTION_VAR_NAMES))\n",
    "row_fmt =\"{:<8.3f} \" * len(emo)\n",
    "print(row_fmt.format(*emo))\n",
    "print(\"Outcomes:\")\n",
    "row_fmt =\"{:<8} \" * len(out)\n",
    "print(row_fmt.format(*helperCode.OUTCOME_VAR_NAMES))\n",
    "row_fmt =\"{:<8.3f} \" * len(out)\n",
    "print(row_fmt.format(*out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoders and Decoders\n",
    "\n",
    "Here we define the neural network encoders and decoders.\n",
    "\n",
    "First some helper modules: The product of experts combines multiple independent gaussians into a single gaussian by averaging their means. The Swish module is an activation function similar to ReLU, but with better performance. (https://arxiv.org/abs/1802.05335)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"\n",
    "    Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param loc: M x D for M experts\n",
    "    @param scale: M x D for M experts\n",
    "    \"\"\"\n",
    "    def forward(self, loc, scale, eps=1e-8):\n",
    "        scale = scale + eps # numerical constant for stability\n",
    "        # precision of i-th Gaussian expert (T = 1/sigma^2)\n",
    "        T = 1. / scale\n",
    "        product_loc = torch.sum(loc * T, dim=0) / torch.sum(T, dim=0)\n",
    "        product_scale = 1. / torch.sum(T, dim=0)\n",
    "        return product_loc, product_scale\n",
    "    \n",
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a deep convolutional generative adversarial network for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|image).\n",
    "    This goes from images to the latent z\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "        #                padding=0, dilation=1, groups=1, bias=True)\n",
    "        # H_out = floor( (H_in + 2*padding - dilation(kernel_size-1) -1) / stride    +1)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
    "            Swish(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Swish(),\n",
    "            nn.Conv2d(128, 256, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            Swish())\n",
    "        # Here, we define two layers, one to give z_loc and one to give z_scale\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 5 * 5, 512), # it's 256 * 5 * 5 if input is 64x64.\n",
    "            #nn.Linear(256 * 9 * 9, 512), # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 5 * 5, 512), # it's 256 * 5 * 5 if input is 64x64.\n",
    "            #nn.Linear(256 * 9 * 9, 512), # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, image):\n",
    "        hidden = self.features(image)\n",
    "        hidden = hidden.view(-1, 256 * 5 * 5) # it's 256 * 5 * 5 if input is 64x64.\n",
    "        #image = image.view(-1, 256 * 9 * 9) # it's 256 * 9 * 9 if input is 100x100.\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden)) #add exp so it's always positive\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "class ImageDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(image|z).\n",
    "    This goes from the latent z to the images\n",
    "    \n",
    "    This is the standard DCGAN architecture.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super(ImageDecoder, self).__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256 * 5 * 5),  # it's 256 * 5 * 5 if input is 64x64.\n",
    "            #nn.Linear(z_dim, 256 * 9 * 9),  # it's 256 * 9 * 9 if input is 100x100.\n",
    "            Swish())\n",
    "        self.hallucinate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            Swish(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False))\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |z_dim|\n",
    "        z = self.upsample(z)\n",
    "        z = z.view(-1, 256, 5, 5) # it's 256 * 5 * 5 if input is 64x64.\n",
    "        #z = z.view(-1, 256, 9, 9) # it's 256 * 9 * 9 if input is 100x100.\n",
    "        # but if 100x100, the output image size is 96x96\n",
    "        image = self.hallucinate(z) # this is the image\n",
    "        return image  # NOTE: no sigmoid here. See train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other modalities, we use a common network structure with two hidden layers for both the encoder and decoder. The networks have two outputs: mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes q(z|input).\n",
    "    This goes from inputs to the latent z\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, input_dim, hidden_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.Linear(input_dim, hidden_dim)\n",
    "        self.z_loc_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, z_dim))\n",
    "        self.z_scale_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, z_dim))\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.net(input)\n",
    "        z_loc = self.z_loc_layer(hidden)\n",
    "        z_scale = torch.exp(self.z_scale_layer(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    define the PyTorch module that parametrizes p(output|z).\n",
    "    This goes from the latent z to the output\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, output_dim, hidden_dim=512):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_dim),\n",
    "            Swish())\n",
    "        self.output_loc_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "        self.output_scale_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = self.net(z)\n",
    "        output_loc = self.output_loc_layer(hidden)\n",
    "        output_scale = torch.exp(self.output_scale_layer(hidden))\n",
    "        return output_loc, output_scale  # NOTE: no softmax here. See train.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal VAE\n",
    "\n",
    "Now we define the multimodal VAE itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the parameters (neural networks), models & guides needed to train a\n",
    "    multimodal variational auto-encoder.\n",
    "    Modified from https://github.com/mhw32/multimodal-vae-public\n",
    "    Multimodal Variational Autoencoder.\n",
    "\n",
    "    @param z_dim: integer\n",
    "                  size of the tensor representing the latent random variable z\n",
    "                  \n",
    "    Currently all the neural network dimensions are hard-coded; \n",
    "    in a future version will make them be inputs into the constructor\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, use_cuda=False):\n",
    "        super(MVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.image_encoder = ImageEncoder(z_dim)\n",
    "        self.image_decoder = ImageDecoder(z_dim)\n",
    "        self.word_encoder = Encoder(z_dim, EMBED_DIM)\n",
    "        self.word_decoder = Decoder(z_dim, EMBED_DIM)\n",
    "        self.rating_encoder = Encoder(z_dim, EMOTION_VAR_DIM)\n",
    "        self.rating_decoder = Decoder(z_dim, EMOTION_VAR_DIM)\n",
    "        self.outcome_encoder = Encoder(z_dim, OUTCOME_VAR_DIM)\n",
    "        self.outcome_decoder = Decoder(z_dim, OUTCOME_VAR_DIM)\n",
    "        self.experts = ProductOfExperts()\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        # using GPUs for faster training of the networks\n",
    "        if self.use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:  # return mean during inference\n",
    "            return mu\n",
    "\n",
    "    def forward(self, word=None, image=None, rating=None, outcome=None):\n",
    "        mu, logvar  = self.infer(word, image, rating, outcome)\n",
    "        # reparametrization trick to sample\n",
    "        z  = self.reparametrize(mu, logvar)\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        word_recon = self.word_decoder(z)\n",
    "        image_recon = self.image_decoder(z)\n",
    "        rating_recon = self.rating_decoder(z)\n",
    "        outcome_recon = self.outcome_decoder(z)\n",
    "        return word_recon, image_recon, rating_recon, outcome_recon, mu, logvar\n",
    "\n",
    "    def infer(self, word=None, image=None, rating=None, outcome=None):\n",
    "        if word is not None:\n",
    "            batch_size = word.size(0)\n",
    "        elif image is not None:\n",
    "            batch_size = image.size(0)\n",
    "        elif rating is not None:\n",
    "            batch_size = rating.size(0)\n",
    "        elif outcome is not None:\n",
    "            batch_size = outcome.size(0)\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        # initialize the universal prior expert\n",
    "        mu, logvar = prior_expert((1, batch_size, self.z_dim),\n",
    "                                   use_cuda=self.use_cuda)\n",
    "        if word is not None:\n",
    "            word_mu, word_logvar = self.word_encoder(word)\n",
    "            mu = torch.cat((mu, word_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, word_logvar.unsqueeze(0)), dim=0)\n",
    "        \n",
    "        if image is not None:\n",
    "            image_mu, image_logvar = self.image_encoder(image)\n",
    "            mu = torch.cat((mu, image_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, image_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if rating is not None:\n",
    "            rating_mu, rating_logvar = self.rating_encoder(rating)\n",
    "            mu = torch.cat((mu, rating_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, rating_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if outcome is not None:\n",
    "            outcome_mu, outcome_logvar = self.outcome_encoder(outcome)\n",
    "            mu     = torch.cat((mu, outcome_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, outcome_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        # product of experts to combine gaussians\n",
    "        mu, logvar = self.experts(mu, logvar)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def model(self, words=None, images=None, ratings=None, outcomes=None):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if words is not None:\n",
    "            batch_size = words.size(0)\n",
    "        elif images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif ratings is not None:\n",
    "            batch_size = ratings.size(0)\n",
    "        elif outcomes is not None:\n",
    "            batch_size = outcomes.size(0)\n",
    "        \n",
    "        with pyro.iarange(\"data\", batch_size):\n",
    "            if outcomes is not None:\n",
    "                # sample from outcome prior, compute p(z|outcome)\n",
    "                outcome_prior_loc = torch.zeros(torch.Size((batch_size, OUTCOME_VAR_DIM)))\n",
    "                outcome_prior_scale = torch.ones(torch.Size((batch_size, OUTCOME_VAR_DIM)))\n",
    "                pyro.sample(\"obs_outcome\", dist.Normal(outcome_prior_loc, outcome_prior_scale).independent(1),\n",
    "                            obs=outcomes.reshape(-1, OUTCOME_VAR_DIM))\n",
    "                \n",
    "                z_loc, z_scale = self.outcome_encoder.forward(outcomes)\n",
    "            else:\n",
    "                # setup hyperparameters for prior p(z)\n",
    "                z_loc = torch.zeros(torch.Size((batch_size, self.z_dim)))\n",
    "                z_scale = torch.ones(torch.Size((batch_size, self.z_dim)))\n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "            # decode the latent code z\n",
    "\n",
    "            word_loc, word_scale = self.word_decoder.forward(z)\n",
    "            # score against actual words\n",
    "            if words is not None:\n",
    "                pyro.sample(\"obs_word\", dist.Normal(word_loc, word_scale).independent(1), \n",
    "                            obs=words.reshape(-1, EMBED_DIM))\n",
    "            \n",
    "            img_loc = self.image_decoder.forward(z)\n",
    "            # score against actual images\n",
    "            if images is not None:\n",
    "                pyro.sample(\"obs_img\", dist.Bernoulli(img_loc).independent(1), \n",
    "                            obs=images.reshape(-1, 3,IMG_WIDTH,IMG_WIDTH))\n",
    "            \n",
    "            rating_loc, rating_scale = self.rating_decoder.forward(z)\n",
    "            # score against actual ratings\n",
    "            if ratings is not None:\n",
    "                pyro.sample(\"obs_rating\", dist.Normal(rating_loc, rating_scale).independent(1), \n",
    "                            obs=ratings.reshape(-1, EMOTION_VAR_DIM))\n",
    "\n",
    "            # return the loc so we can visualize it later\n",
    "            return word_loc, img_loc, rating_loc\n",
    "    \n",
    "    def guide(self, words=None, images=None, ratings=None, outcomes=None):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"mvae\", self)\n",
    "        \n",
    "        batch_size = 0\n",
    "        if words is not None:\n",
    "            batch_size = words.size(0)\n",
    "        elif images is not None:\n",
    "            batch_size = images.size(0)\n",
    "        elif ratings is not None:\n",
    "            batch_size = ratings.size(0)\n",
    "        elif outcomes is not None:\n",
    "            batch_size = outcomes.size(0)\n",
    "            \n",
    "        with pyro.iarange(\"data\", batch_size):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "                        \n",
    "            # initialize the prior expert\n",
    "            # the additional dimension (1) is to \n",
    "            z_loc = torch.zeros(torch.Size((1, batch_size, self.z_dim)))\n",
    "            z_scale = torch.ones(torch.Size((1, batch_size, self.z_dim)))\n",
    "            if self.use_cuda:\n",
    "                z_loc, z_scale = z_loc.cuda(), z_scale.cuda()\n",
    "            \n",
    "            # figure out the elbo loss? encoder/decoder?\n",
    "            if outcomes is not None:\n",
    "                outcome_z_loc, outcome_z_scale = self.outcome_encoder.forward(outcomes)\n",
    "                z_loc = torch.cat((z_loc, outcome_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, outcome_z_scale.unsqueeze(0)), dim=0)\n",
    "\n",
    "            if words is not None:\n",
    "                word_z_loc, word_z_scale = self.word_encoder.forward(words)\n",
    "                z_loc = torch.cat((z_loc, word_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, word_z_scale.unsqueeze(0)), dim=0)                \n",
    "                \n",
    "            if images is not None:\n",
    "                image_z_loc, image_z_scale = self.image_encoder.forward(images)\n",
    "                z_loc = torch.cat((z_loc, image_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, image_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            if ratings is not None:\n",
    "                rating_z_loc, rating_z_scale = self.rating_encoder.forward(ratings)\n",
    "                z_loc = torch.cat((z_loc, rating_z_loc.unsqueeze(0)), dim=0)\n",
    "                z_scale = torch.cat((z_scale, rating_z_scale.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            z_loc, z_scale = self.experts(z_loc, z_scale)\n",
    "            # sample the latent z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Here we set up the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "class Args:\n",
    "    learning_rate = 5e-5\n",
    "    num_epochs = 500 #1000\n",
    "    hidden_layers = DEFAULT_HIDDEN_DIMS\n",
    "    z_dim = DEFAULT_Z_DIM\n",
    "    seed = 10\n",
    "    cuda = False\n",
    "    visdom_flag = False\n",
    "    #visualize = True\n",
    "    #logfile = \"./tmp.log\"\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# setup the VAE\n",
    "mvae = MVAE(z_dim=args.z_dim, use_cuda=args.cuda)\n",
    "#vae = VAE(z_dim=args.z_dim, use_cuda=args.cuda)\n",
    "\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": args.learning_rate}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(mvae.model, mvae.guide, optimizer, loss=Trace_ELBO())\n",
    "#svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we actually train the MVAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_elbo = []\n",
    "# training loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch returned\n",
    "    # by the data loader\n",
    "    for batch_num, (_, words, faces, ratings, outcomes) in enumerate(word_outcome_emotion_loader):\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if args.cuda:\n",
    "            faces = faces.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        #print(\"Batch: \", batch_num, \"out of\", len(train_loader))\n",
    "        #epoch_loss += svi.step(faces)\n",
    "        #epoch_loss += svi.step(ratings)\n",
    "        if len(words.shape) == 1:\n",
    "            words = None\n",
    "        if len(faces.shape) == 1:\n",
    "            faces = None\n",
    "        if len(ratings.shape) == 1:\n",
    "            ratings = None\n",
    "        if len(outcomes.shape) == 1:\n",
    "            outcomes = None\n",
    "        epoch_loss += svi.step(words, faces, ratings, outcomes)\n",
    "        \n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer_train = len(face_outcome_emotion_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can save or load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "savemodel = True\n",
    "if savemodel:\n",
    "    pyro.get_param_store().save('models/word_mvae_pretrained.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmodel = True\n",
    "if loadmodel:\n",
    "    pyro.get_param_store().load('models/word_mvae_pretrained.save')\n",
    "    pyro.module(\"mvae\", mvae, update_module_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the reconstructed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag whether to evaluate labelled or non-labelled examples\n",
    "eval_training = False\n",
    "# Training set of words\n",
    "train_words = ['awesome', 'cool', 'damn', 'dang', 'man', 'meh', 'oh', 'wow', 'yay', 'yikes']\n",
    "# Test set of words\n",
    "test_words = ['amazing', 'nope', 'nice', 'wonderful', 'jeez', 'gah', 'shit', 'sigh', 'ugh']\n",
    "\n",
    "if eval_training:\n",
    "    # Use training set as samples\n",
    "    samples = []\n",
    "    df = word_emotion_outcome_dataset.expdata\n",
    "    for w in train_words:\n",
    "        # Lookup emotion ratings for each word\n",
    "        df_ratings = df[df['utterance']==w].loc[:,\"happy\":\"disapp\"]\n",
    "        # Average across all observations\n",
    "        ratings = df_ratings.mean(axis=0).values\n",
    "        samples.append((w, torch.from_numpy(embeddings[w]), ratings))\n",
    "else:\n",
    "    # Use test set as samples\n",
    "    samples = []\n",
    "    for w in test_words:\n",
    "        samples.append((w, torch.from_numpy(embeddings[w]), 0))    \n",
    "\n",
    "# Number of nearest neighbors to the reconstructed vector to find\n",
    "k_neighbors = 0 # 4\n",
    "    \n",
    "print(\"Reconstruction similarity, neighbors and emotion ratings\")\n",
    "for word, embed, ratings in samples:\n",
    "    # Reconstruct the data\n",
    "    (word_recon, image_recon, rating_recon, outcome_recon, mu, logvar) =\\\n",
    "        mvae.forward(embed, None, None, None)\n",
    "    # Find cosine similarity\n",
    "    sim = cosine_sim_torch(embed, word_recon).detach().numpy()\n",
    "\n",
    "    if k_neighbors > 0:\n",
    "        embed_np = recon_embed.detach().numpy()\n",
    "        nb_words = heapq.nlargest(k_neighbors, exclamations,\n",
    "                                  key=lambda x: cosine_sim_np(embed_np, embeddings[x]))\n",
    "        \n",
    "    # Print reconstruction similarity\n",
    "    print(\"{:8} : {:10}\".format(word, sim))\n",
    "    if k_neighbors > 0:\n",
    "        print(\"neighbors: \", nb_words)\n",
    "    str_row_fmt =\"{:<8.8} \" * len(rating_recon.detach().numpy())\n",
    "    print(str_row_fmt.format(*EMOTION_VAR_NAMES))\n",
    "    num_row_fmt =\"{:<8.1f} \" * len(rating_recon.detach().numpy())\n",
    "    # Print average of observed raitings if evaluating training\n",
    "    if eval_training:\n",
    "        print(num_row_fmt.format(*(ratings*8+1)))     \n",
    "    print(num_row_fmt.format(*(rating_recon.detach().numpy()*8+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a sample observation\n",
    "img1, emo1, out1 = face_outcome_emotion_dataset[5]\n",
    "print(\"Sample Observation: \")\n",
    "print(helperCode.EMOTION_VAR_NAMES)\n",
    "print(emo1)\n",
    "print(helperCode.OUTCOME_VAR_NAMES)\n",
    "print(out1)\n",
    "Image.fromarray(helperCode.TensorToPILImage(img1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvae.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Written by: Desmond Ong (desmond.c.ong@gmail.com), Harold Soh (hsoh@comp.nus.edu.sg), Mike Wu (wumike@stanford.edu)\n",
    "\n",
    "References:\n",
    "\n",
    "Pyro [VAE tutorial](http://pyro.ai/examples/vae.html)\n",
    "\n",
    "Wu, M., & Goodman, N. D. (2018). Multimodal Generative Models for Scalable Weakly-Supervised Learning. To appear, NIPS 2018, https://arxiv.org/abs/1802.05335\n",
    "Repo here: https://github.com/mhw32/multimodal-vae-public\n",
    "\n",
    "DCGAN https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "Hoffman, M. D., Blei, D. M., Wang, C., & Paisley, J. (2013). Stochastic variational inference. *The Journal of Machine Learning Research*, 14(1), 1303-1347.\n",
    "\n",
    "Kingma, D. P., Mohamed, S., Rezende, D. J., & Welling, M. (2014). Semi-supervised learning with deep generative models. In *Advances in Neural Information Processing Systems*, pp. 3581-3589. https://arxiv.org/abs/1406.5298\n",
    "\n",
    "Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. Auto-Encoding Variational Bayes. In *The International Conference on Learning Representations*. https://arxiv.org/abs/1312.6114\n",
    "\n",
    "\n",
    "Narayanaswamy, S., Paige, T. B., van de Meent, J. W., Desmaison, A., Goodman, N. D., Kohli, P., Wood, F. & Torr, P. (2017). Learning Disentangled Representations with Semi-Supervised Deep Generative Models. In *Advances in Neural Information Processing Systems*, pp. 5927-5937. https://arxiv.org/abs/1706.00400\n",
    "\n",
    "Data from https://github.com/desmond-ong/affCog, from the following paper:\n",
    "\n",
    "Ong, D. C., Zaki, J., & Goodman, N. D. (2015). Affective Cognition: Exploring lay theories of emotion. *Cognition*, 143, 141-162."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
